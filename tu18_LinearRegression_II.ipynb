{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2d5b259",
   "metadata": {},
   "source": [
    "# Linear Regression II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a30b22",
   "metadata": {},
   "source": [
    "Learning objectives:\n",
    "\n",
    "Learn basic measures of quality of fit and variance explained:\n",
    "- Mean Squared Errors\n",
    "- R2\n",
    "- Total Sum of Squares\n",
    "- Residual Sum of Squares\n",
    "- Explained Sum of Squares\n",
    "- Linear Regression with multiple parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636a8560",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Linear regression *recap*. \n",
    "\n",
    "In the last tutorial we went over linear regression using *numpy*'s *polyval* and *polyfit*.\n",
    "\n",
    "Linear regression is used to model the relationship between a dependent variable and one or more independent variables. It assumes that there is a linear relationship between the independent variable(s) and the dependent variable. \n",
    "\n",
    "The goal of linear regression is to find the best-fit model that minimizes the difference between the predicted values and the actual values. Linear regression is important because it can help us to understand and predict the relationship between two or more variables. The term **linear** in Linear regression does not refer only to a line, but it applies to any polynomial! Linear here is referred to the parameters of the model and not the model *per se*!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e47cbf1",
   "metadata": {},
   "source": [
    "\n",
    "In the previous tutorial we have learned how to fit models via linear regression by using `numpy`. Here we will learn a little bit more about fitting linear regression models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3acdf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate random data\n",
    "np.random.seed(123)\n",
    "x = np.random.rand(50)\n",
    "y = 2*x + 0.5*np.random.randn(50)\n",
    "\n",
    "# Fit a linear regression model\n",
    "coeffs = np.polyfit(x, y, 1)\n",
    "y_pred = np.polyval(coeffs, x)\n",
    "\n",
    "# Print the coefficients\n",
    "print(\"Intercept:\", coeffs[0])\n",
    "print(\"Coefficient:\", coeffs[1])\n",
    "\n",
    "# Compute SSE\n",
    "sse = sum((y-y_pred)**2)\n",
    "print(\"SSE:\", sse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79596c28",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### <span style=\"color:blue\">Exercise</span> \n",
    "\n",
    "- Generate your own data of shape `(10,)`.\n",
    "- Fit a line and estimate the SSE\n",
    "- Make a single figure and plot, data, and line in different colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74158dcc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Linear regression using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4210ee",
   "metadata": {},
   "source": [
    "The same operations of fitting and evaluating the fit of a regression model can also be implemented using a much more powerful set of toosl implemented in the machine learning library `scikit-learn`. `scikit-learn` has module dedicated to linear regression models called `LinearRegression`. \n",
    "\n",
    "Let's import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3927e662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03a174f",
   "metadata": {},
   "source": [
    "We can implement the operations shown about using `polyval` and `polyfit` using `LinearRegression` using the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a44dacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear model\n",
    "model = LinearRegression()\n",
    "\n",
    "# The fit method in LinearRegression only acceps predictors (x) as matrices. \n",
    "# So we need to reshape our array:\n",
    "X = np.array(x).reshape(-1, 1)\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# The coefficients can be extracted from the fit model as follows:\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "print(\"Coefficient:\", model.coef_[0])\n",
    "\n",
    "# Compute SSE\n",
    "sse = sum((y-y_pred)**2)\n",
    "print(\"SSE:\", sse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b781929",
   "metadata": {},
   "source": [
    "OK, besides the ideosyncracy of how `LinearRegression` accepts `x`, that was not very different. Instead of  using `polyfit` and `polyval`, we used `model.fit` and `model.predict` and the results (parameers and MSE) were identical. Good.\n",
    "\n",
    "Now, `LinearRegression` might seem a little bit more complicated because, oh well, it is more complicated but also much more powerful! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91852064",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### <span style=\"color:blue\">Exercise</span> \n",
    "\n",
    "- Generate a new data set of shape `(12,)`.\n",
    "- Fit a line using `scikit-learn.linear_model` and estimate the SSE\n",
    "- Make a single figure and plot, data, and line in different colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef91437",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quality of fit metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2352c0f",
   "metadata": {},
   "source": [
    "Linear regession is more generally referred to as *Ordinary Linear Square Regression* or *OLS* Regression. \n",
    "\n",
    "This is because the approach in regression is to minimise the sum of square errors (SSEs) between the data and the prediction of a model. The parameters of the model are adjusted so as to reduce the SSE and eventually minimize it.\n",
    "\n",
    "We have seen before how to compute the SSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730f67f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum((y-y_pred)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a2099d",
   "metadata": {},
   "source": [
    "In addition to SSE there are other measures of error important to learn about. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5079ffec",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "When fitting OLS regression models we attempt to explain some proportion of the variability in the data with a model. More specifically, we try to explain some proportion fo the *variance* in the data using the model. So models are generally judged by the proportion of variance in the data that they can explain. \n",
    "\n",
    "The proportion of variance explained is a measure that describes the amount of variation in the dependent variable ($y$) that can be explained by the independent variable(s) ($x$) in a statistical model, such as a linear regression model.\n",
    "\n",
    "When we fit a regression model, we are trying to find a line (or curve) that best represents the relationship between the independent variable(s) and the dependent variable. The amount of variation in the dependent variable that can be explained by the independent variable(s) is determined by the fit of the regression line to the data points.\n",
    "\n",
    "In the context of linear regression, the total sum of squares ($TSS$) can be decomposed into two components: \n",
    "* the explained sum of squares ($ESS$) and \n",
    "* the residual sum of squares ($RSS$ or as called until now, the sum of squared error, $SSE$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722f47ea",
   "metadata": {},
   "source": [
    "The explained sum of squares ($ESS$) is the sum of squares of the difference between the predicted values of the dependent variable and the mean of the dependent variable. It represents the amount of variability in the dependent variable that is explained by the independent variable(s) in the model.\n",
    "\n",
    "$ESS = Σ(ŷi - ȳ)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1030dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ESS = sum((y_pred - np.mean(y))**2)\n",
    "print(ESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59c6aac",
   "metadata": {},
   "source": [
    "The residual sum of squares ($RSS$, a.k.a., $SSE$) is the sum of squares of the difference between the predicted values of the dependent variable and the actual values of the dependent variable. It represents the amount of variability in the dependent variable that is not explained by the independent variable(s) in the model.\n",
    "\n",
    "$RSS = Σ(yi - ŷi)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9bc72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSS = sum((y - y_pred)**2) #a.k.a. SSE\n",
    "print(RSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41f8d8d",
   "metadata": {},
   "source": [
    "The total sum of squares ($TSS$) is the sum of squares of the difference between the actual values of the dependent variable and the mean of the dependent variable. It represents the total amount of variability in the dependent variable.\n",
    "\n",
    "$TSS = Σ(yi - ȳ)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0da2c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TSS = sum((y - np.mean(y))**2)\n",
    "print(TSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e124b35",
   "metadata": {},
   "source": [
    "where $y_i$ is the actual value of the dependent variable, $ŷ_i$ is the predicted value of the dependent variable, and $ȳ$ is the mean of the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ad8832",
   "metadata": {},
   "source": [
    "Note that $TSS = Σ(RSS + ESS)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738d4a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([TSS, RSS+ESS])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21df016b",
   "metadata": {},
   "source": [
    "So far we have used only the SSE to copute the quality of fit of a model. There are several alternatives to RSS (a.k.a., SSE) that can be used to estimate the quality of fit of a model. A few commonly used ones are:\n",
    "\n",
    "* Mean squared error (MSE): MSE is calculated as SSE divided by the number of degrees of freedom in the model. It is a measure of the average squared difference between the predicted values of the dependent variable and the actual values, and is often used as a measure of the overall goodness of fit of a model.\n",
    "\n",
    "* Root mean squared error (RMSE): RMSE is the square root of MSE and is often used as a more interpretable measure of the overall goodness of fit of a model. RMSE has the same units as the dependent variable and is more easily interpretable than MSE.\n",
    "\n",
    "* Mean absolute error (MAE): MAE is a measure of the average absolute difference between the predicted values of the dependent variable and the actual values. It is less sensitive to outliers than SSE and can be more robust in the presence of extreme values.\n",
    "\n",
    "* Coefficient of determination (R²): R² is a measure of the proportion of variance in the dependent variable that is explained by the independent variables in the model. It ranges from 0 to 1, with higher values indicating a better fit between the model and the observed data. $R² = 1 - (SSE / TSS)$\n",
    "\n",
    "Each one of these metrics is useful in different situations. Others also exist such as the K-L Divergence or Akaiake Information Criteria (AIK) or Baeysian Information Cirteria (BIC), we will cover some of these only in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52bcabd",
   "metadata": {},
   "source": [
    "\n",
    "`scikit-learn` provides a convenient way to compute several goodness of fit  metrics to evaluate model performance. The module `sklearn.metrics` can be imported and submodules within it contain estimators of the goodness of fit of models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62263d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_squared_error, r2_score\n",
    " \n",
    "mae  = mean_absolute_error(y_true=y,y_pred=y_pred)\n",
    "mse  = mean_squared_error(y_true=y,y_pred=y_pred) #squared=True\n",
    "rmse = mean_squared_error(y_true=y,y_pred=y_pred,squared=False)\n",
    "r2   = r2_score(y_true=y,y_pred=y_pred)\n",
    " \n",
    "print(\"Mean Absolute Error (MAE):\",mae)\n",
    "print(\"Mean Squared Error (MSE):\",mse)\n",
    "print(\"Root-Mean Squared Error (RMSE):\",rmse)\n",
    "print(\"Coefficient of Determination (R²):\",r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af96d84",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### <span style=\"color:blue\">Exercise</span> \n",
    "\n",
    "- Generate a new data set of shape `(15,)`.\n",
    "- Fit a line using `scikit-learn.linear_model` and estimate the SSE\n",
    "- Use `scikit-learn.metrics` to estimate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0559c5",
   "metadata": {},
   "source": [
    "### Linear regression using scikit-learn (generalized linear regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fae1769",
   "metadata": {},
   "source": [
    "So, far we have used `scikit-learn`'s `LinearRegression` uniquely to predict `n` `y` variables from `n` `x` variables.\n",
    "\n",
    "Yet, in practice we can think situations where we might have multiple variables (say `n x m` variables) and we would like to use them to predict a single set of `n` variables.\n",
    "\n",
    "For example imagine the case of `m` repeated measures of `n` values and wanting to predict corresponding `n` values of another variable.\n",
    "\n",
    "`LinearRegression` allows us to set up this type of modelling. This is the reason why the `X` variables must alswyas be 2D and above we had to make sure it was a 2D array.\n",
    "\n",
    "To work this example, we will use one of the datasets that come with `scikit learn`, the Boston Housing database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0280a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e435ef7",
   "metadata": {},
   "source": [
    "To explore the dataset take a look at the Headers and Dictionary Keys. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0467e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston_dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96727a73",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "* `data`: contains the information for various houses\n",
    "* `target`: prices of the house\n",
    "* `feature_names`: names of the features\n",
    "* `DESCR`: describes the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ad1eb0",
   "metadata": {},
   "source": [
    "The dataset contains a series of attributes or features (variables) measured along different dimensions. \n",
    "\n",
    "Take a look at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39738705",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston_dataset.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190228d0",
   "metadata": {},
   "source": [
    "The last variable `MEDV` (or median value) is our interest. It is the median value of homes in thousands of dollars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4e3511",
   "metadata": {},
   "source": [
    "The dataset contains `.data` and `.target`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4687b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston_dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b12a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston_dataset.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fe7acb",
   "metadata": {},
   "source": [
    "For convenience we are going to create a smalled table of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f46e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947a26e9",
   "metadata": {},
   "source": [
    "We can take a look at the median house value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccb20eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(boston_dataset.target, bins=56)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51efbfdb",
   "metadata": {},
   "source": [
    "We can explore the relationship between some of the features in the data and the target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb08932",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = boston['RM']\n",
    "y = boston_dataset.target\n",
    "plt.scatter(x, y, marker='o')\n",
    "plt.xlabel('The average number of rooms per dwelling (RM)')\n",
    "plt.ylabel('Median House Values (MEDV)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9e2b82",
   "metadata": {},
   "source": [
    "OK, it looks like there are features (like \"RM\") that have a relationship with the Median House Values. Let's try another feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990f950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = boston['LSTAT']\n",
    "y = boston_dataset.target\n",
    "plt.scatter(x, y, marker='o')\n",
    "plt.xlabel('The lower status of the population (LSTAT)')\n",
    "plt.ylabel('Median House Values (MEDV)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6516d7e",
   "metadata": {},
   "source": [
    "Also, a relationship. So, it looks like multiple features in the dataset have a relationship with the target variable (the median house value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4624c35",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### <span style=\"color:blue\">Exercise</span> \n",
    "\n",
    "- Explore the relationship between the target variable and two additional features of your choice. Make a plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83cfe39",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "It looks like multiple features have some relationship with the median house value in Boston. \n",
    "\n",
    "So, it makes sense to think that a linear combination of all these variables should predict in some way the median house value. This is a case in which `m` variables (features) predict alltogether a target variable.\n",
    "\n",
    "We will use `LinearRegression` to experiment with fitting a linear model where `m` features predict a single variable.\n",
    "\n",
    "First let's organize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50daa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dependent and independent variables from the data set\n",
    "X = boston_dataset.data\n",
    "y = boston_dataset.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d993df",
   "metadata": {},
   "source": [
    "Second, let's fit the linear regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52e732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_linear_regression = LinearRegression()\n",
    "housing_linear_regression.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70148bf9",
   "metadata": {},
   "source": [
    "Third, we will use the model to predict the data, the median house value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf4bba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = housing_linear_regression.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec789f7",
   "metadata": {},
   "source": [
    "Finally, we will compare using a plot the predicted and measured Median House Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec287ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_array = np.array(y).reshape(-1, 1) \n",
    "y_pred_array = np.array(y_pred).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928b74b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = y_data_array\n",
    "y = y_pred_array\n",
    "plt.scatter(x, y, marker='o')\n",
    "plt.xlabel('Predicted: Median House Values (MEDV)')\n",
    "plt.ylabel('Measured: Median House Values (MEDV)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce55b08",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### <span style=\"color:blue\">Exercise</span> \n",
    "\n",
    "- Explain in your own words what you see in the previos Figure.\n",
    "- Describe what the above experiment did\n",
    "- How many features where in our model?\n",
    "- How good was the quality of the fit (what was the R2 and MSE)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f41dae4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
