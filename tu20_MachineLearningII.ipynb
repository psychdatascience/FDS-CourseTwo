{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "675936fe-2793-4030-9677-157a8cc476af",
   "metadata": {},
   "source": [
    "# Machine Learning II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc19481e-0e2b-490f-bb03-b32a57cb1faf",
   "metadata": {},
   "source": [
    "Today, we will cover two of the more common machine learning algorithms, as well as a powerful technique for reducing the dimensionality (the number of predictor variables) of the data prior to applying a machine learning algorithm to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472fca53-4b43-4bc4-9485-af9e055c797c",
   "metadata": {},
   "source": [
    "Specifically, this tutorial will cover\n",
    "\n",
    "- Support Vector Machines\n",
    "- Using Principal Components Anaysis to simplfy predictor variables\n",
    "- Gaussian Naive Bayes Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e16efd-469c-4256-9c1b-951051fc5dbd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8727954-6efc-4280-a51b-13a89a2d8812",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c0f57-42a4-471d-a59d-2ca5ff050fea",
   "metadata": {},
   "source": [
    "A Support Vector Machine (SVM) is a supervised machine learning algorithm primarily used for classification tasks. It works by finding the optimal decision boundary that best separates the data into different classes. The key concept behind SVM is to maximize the \"margin\" (distance) between classes, which is defined as the distance between the decision boundary and the nearest data points from each class. These nearest points are called *support vectors*, and they determine the position and orientation of the boundary.\n",
    "\n",
    "SVM can handle both linearly separable and non-linearly separable data. For linearly separable data, the algorithm finds a straight-line boundary that best separates the classes. However, for non-linearly separable data, SVM uses a technique called the kernel trick. The kernel trick involves transforming the input data into a higher-dimensional space where it becomes linearly separable, allowing the SVM to find the optimal separating boundary.\n",
    "\n",
    "Here's a step-by-step overview of how SVM works:\n",
    "\n",
    "Determine the optimal boundary: Find the boundary that best separates the classes by maximizing the margin between them. This is achieved by minimizing a cost function that considers both the margin size and the classification error.\n",
    "\n",
    "Identify support vectors: Find the data points that lie closest to the decision boundary, as they are critical in defining the optimal boundary.\n",
    "\n",
    "Make predictions: For a new input data point, determine which side of the decision boundary it lies on.\n",
    "\n",
    "SVM is known for its effectiveness in high-dimensional spaces and robustness against overfitting, making it suitable for various applications, including image classification, text categorization, and bioinformatics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a60b821-66fe-418c-a285-8986fc4676b8",
   "metadata": {},
   "source": [
    "First, let's import our needed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ac84a352-a47c-47fd-bdeb-cead8015ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f6b3ba-6033-4432-8baf-fc0c77915f96",
   "metadata": {},
   "source": [
    "Now well create some toy data to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3cdc264f-a714-4b1a-844a-77a775640d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two 2D blobs of data\n",
    "X, y = make_blobs(n_samples=300, centers=2, \n",
    "                  random_state=42, cluster_std=1.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185e9e2e-7a24-4729-82d4-405d09793652",
   "metadata": {},
   "source": [
    "Now let's look at our toy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be5fc34-a659-4458-8303-7fc4b0792dbc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073df502-e275-49eb-9b00-fa48b4ead1ed",
   "metadata": {},
   "source": [
    "Make a scatter plot of our new toy data in the cell below. Use the target variable, `y`, to color code the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8815185-153e-414b-8121-dec62fc7cf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the blobs of data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93213fe8-dec5-4e48-90f5-2d8f2586a261",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd866541-5918-4369-8340-19e24dd74c71",
   "metadata": {},
   "source": [
    "These two \"blobs\" of data look like they are clearly separable. So let's make a classifier! The following steps should look familiar. They follow our basic machine learning workflow, which is:\n",
    "\n",
    "- load the needed things from numpy, matplotlib and, the problem-specific tools from scikit-learn (already done)\n",
    "- get the data and wrangle it into shape if necessary (already done)\n",
    "- split the data into training and test sets\n",
    "- train the machine learning algorithm\n",
    "- evaluate the performance of the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e3defd-2d36-433f-b0fa-41941f6c9be2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce5d074-5855-4719-a758-76edaa398339",
   "metadata": {},
   "source": [
    "So now let's split the data into training and test sets. Use the cell below to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "91fff4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (70% training, 30% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa3f86e-ff4c-425e-8a9e-ba9775e9ea27",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d6b78-01e3-4e8c-b801-f00c9650213f",
   "metadata": {},
   "source": [
    "Now we'll create a support vector classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "84120274-15e0-48e1-8f1c-c02a3b50814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Support Vector Machine (SVM) classifier and train it on the training data\n",
    "svm = SVC(kernel='linear', C=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75a3aaa-ab76-4f12-afed-00b70e69e804",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee37016-f991-4e85-9009-305db6393b91",
   "metadata": {},
   "source": [
    "And now we can train it on out training data. Use the cell below to so this. As always in scikit-learn, we use the `.fit()` method to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df95b85b-760a-4bdb-becf-dc8bf9de42b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f2b2cc1-81f1-4ca3-99bc-bc4f408076ad",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfcf7aa",
   "metadata": {},
   "source": [
    "Now let's make a plot of the training data and the decision boundary that the SVM found. Don't worry if you don't understand all the plotting code below; it's a bit next-level. But do look through it and see how much you can figure out, and if you're curious about any of it, just ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5626cf12-03f6-4914-bd24-2ac2b9d7c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of the test data showing the decision boundary\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis')\n",
    "\n",
    "# Create the decision boundary\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = svm.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# Plot the decision boundary\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], \n",
    "           alpha=0.5, linestyles=['--', '-', '--'])\n",
    "\n",
    "plt.title(\"Training Data with SVM Decision Boundary\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614703b9-fea0-422e-b51b-9608d75c920b",
   "metadata": {},
   "source": [
    "Here, we can see the decision boundary (the solid line) and the margins (dashed lines) passing through the support vectors (the closest points from each group to the decision boundary).\n",
    "\n",
    "Now that we have our decision boundary, we can start using our SVM on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7178a469-5276-4a4f-b419-9541d8371414",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320f2af9-9e65-460f-882d-62296681a079",
   "metadata": {},
   "source": [
    "Use the cell below to generate the predicted `y_pred` data based upon `X_test`. It's the same as we've done before: use the `.predict()` method of our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dbd9f8-9a8b-4bae-8842-7502e9e1c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1d3a01-0568-4c0c-9056-286315d23128",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8aae47-0e92-4a80-a619-12a0b8ffe8c0",
   "metadata": {},
   "source": [
    "The fun part! Let's plot the test data showing the predicted category (by color) and the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab1e40a-9d41-4a7c-95bb-d1a5b392c23e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scatter plot of the test data showing the decision boundary\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='viridis')\n",
    "\n",
    "# Create the decision boundary\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = svm.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# Plot the decision boundary\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "\n",
    "plt.title(\"Test Data with SVM Decision Boundary\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5b272b",
   "metadata": {},
   "source": [
    "Here, we can see that that, even though some of the points fell within the margin, the classification was perfect.\n",
    "\n",
    "For completeness though, let's look at some standard evaluation metrics for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c41fa6-cf46-4160-9170-dba55b39342d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447211d3-acb5-4eb2-9d94-a53f69b0acba",
   "metadata": {},
   "source": [
    "Use the cell below to print the confusion matrix, the classification report, and the accuracy score (as we did in the previous tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "09356db5-293d-47fe-947b-d825540b8c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the classifier's performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d81da55-76ef-465c-8b72-6d9cd03025f9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b8a432-6676-42a3-b130-b83c8810bcd2",
   "metadata": {},
   "source": [
    "## Dimension Reduction via PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f61e8e4-1693-4142-9592-3727f79bd2c1",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used in machine learning, statistics, and data analysis to transform a high-dimensional dataset into a lower-dimensional space while retaining as much information as possible. The primary goal of PCA is to identify and project the data onto the most significant directions (principal components) in the feature space, where the variance of the data is maximized.\n",
    "\n",
    "Here are the basic steps of PCA:\n",
    "\n",
    "**Standardization**: The first step is to standardize the dataset by scaling each feature to have a mean of 0 and a standard deviation of 1. This ensures that all features have equal importance in the analysis and prevents larger-scale features from dominating the results. \n",
    "\n",
    "Simply put, it makes the data unit-independent by converting all the data to their Z-scores. This means that, for example, a questionaire with scores ranging from 0 to 100 won't be weighted more than a hormone measure that ranges from 0 to 5.\n",
    "\n",
    "**Covariance Matrix**: PCA computes the covariance matrix of the standardized dataset, which captures the linear relationships between the features. The covariance matrix helps determine the directions in which the variance of the data is maximized.\n",
    "\n",
    "This a fancy way of saying that we compute the correlation between all the variables. You can think of the covariance matrix as just a numerical version of a pair-pair plot.\n",
    "\n",
    "**Eigenvalues and Eigenvectors**: The next step is to compute the eigenvalues and eigenvectors of the covariance matrix. Eigenvectors represent the directions of the principal components, while eigenvalues represent the magnitude of the variance along each principal component. The eigenvector corresponding to the largest eigenvalue represents the direction with the highest variance, and so on.\n",
    "\n",
    "We can think of the \"Eigenvalues\" as being proportional to \"variance explained\" or $R^2$ - if the first two, say, explain most of the variance, then we can safely use the first two principal components to do our categorization.\n",
    "\n",
    "**Projection onto Principal Components**: Finally, the original dataset is projected onto the principal components (eigenvectors) to obtain the transformed data in the reduced-dimensional space. The number of principal components retained is typically determined by the desired level of variance preservation or the specific problem requirements.\n",
    "\n",
    "In this step, we just effectively re-plot our data with the first and second principal components are the x and y axes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404cefa5-3068-418b-951c-8754251451a8",
   "metadata": {},
   "source": [
    "PCA has numerous applications, such as data visualization, noise reduction, feature extraction, and improving the efficiency of other machine learning models by reducing the input dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef620256-124e-47ce-82db-e19585f89901",
   "metadata": {},
   "source": [
    "### PCA on the Iris data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f512ce-0566-4632-851d-5a5c790a487d",
   "metadata": {},
   "source": [
    "Let's do a PCA on the iris data! The iris data has 4 predictors, and we're going to use PCA to boil these down into 2 predictors - \"principal components\" - that we can then use to do a classification of iris species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800541b6-ee5e-4bfd-931c-b557759ddd67",
   "metadata": {},
   "source": [
    "First, let's import out libraries so that the cells below are stand-alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9d27ea1c-7d0d-48ec-b7a4-8bc9323416f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2730c06-69fe-4a3c-860d-3b5105d1d319",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d469cb9-6066-499d-9c83-33a475b77cf1",
   "metadata": {},
   "source": [
    "In the cell below, load the iris dataset. Make your `X` predictor variable equal to the iris data, and your `y` target variable equal to iris target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b1886d84-2cd8-4c8b-bbaf-ac42599e65d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1c2773-0ef8-4d00-a1e9-42a1b8717cfa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7b633c-5340-4fdb-b7d3-680b60a7280d",
   "metadata": {},
   "source": [
    "Now we'll do a PCA on the data to reduce the 4 dimensional data to 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f9914eaa-29b2-4663-ada3-f6857998f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on the Iris dataset, \n",
    "# keeping only the first two principal components\n",
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cea1fd4-5b59-4864-8aa7-b441ffc3a54d",
   "metadata": {},
   "source": [
    "Now we'll make our new shifted / rotated predictor variables based on the PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a631a9ee-922c-4e25-baed-99bedf316197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the new predicted data based on the PCA rotation.\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e38f09a-fa47-4ddc-a415-5bb9527e7d40",
   "metadata": {},
   "source": [
    "Look at our new PCA transformed data! Look through the plotting code and make sure you understand what it's doing. Ask about the `zip` function if you don't understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf6b4da-55b8-41e1-84bb-2312f8ddbe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of the first two principal components \n",
    "# with color coding for species\n",
    "colors = ['red', 'green', 'blue']\n",
    "species = iris.target_names\n",
    "\n",
    "for i, color, target_name in zip(range(3), colors, species):\n",
    "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], \n",
    "                color=color, label=target_name)\n",
    "\n",
    "plt.xlabel(\"First Principal Component\")\n",
    "plt.ylabel(\"Second Principal Component\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"PCA on Iris dataset: First two principal components\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec5cf35",
   "metadata": {},
   "source": [
    "If you refer back to the pair plot of the iris data we did in the previous tutorial, you will see that it looks a bit like a few of the plots, but it doesn't look *exactly* like any of them. That's because each data point's coordinate (its pair of values) is a *weighted combination* of its original values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d8e540",
   "metadata": {},
   "source": [
    "Now that we have boiled down the original 4 iris variables into just two values per observation (the 1st and 2nd principal components), let's learn another type of classifier using the new data. (Note that you could also use a k nearest neighbor or any other classifier here – the PCA itself doesn't care what you do after it, and classifiers don't care what you've done the data before you use them)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cffb3ff-5103-46c2-a22c-2c89a4a25a84",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aab8dc-7e60-48a6-8148-8b77ff890584",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f6a75e",
   "metadata": {},
   "source": [
    "A Naive Bayes classifier is a probabilistic machine learning model based on Bayes' theorem, which is used for classification tasks. It's called \"naive\" because it makes the strong assumption that the features in the dataset are conditionally independent, given the class label. Despite this simplification, Naive Bayes classifiers often perform well in practice and are particularly suited for text classification, spam filtering, and other problems with large feature spaces and relatively simple relationships between features and classes.\n",
    "\n",
    "Bayes' theorem relates the conditional probabilities of the features and the class labels, and it's formulated as:\n",
    "\n",
    "$P(y | X) = P(X | y) * P(y) / P(X)$\n",
    "\n",
    "where:\n",
    "\n",
    "P(y | X) is the **posterior probability** of the class label y, given the corresponding features in X.\n",
    "\n",
    "P(X | y) is the **likelihood**, which is the probability of the set of features, X, given the class label y.\n",
    "\n",
    "P(y) is the **prior probability** of the class label y, which represents the overall frequency of each class in the dataset.\n",
    "\n",
    "P(X) is the evidence, which is the probability of the entire set of features, X, without regard to any target values, y. In other words, it is an \"unconditional probability\". *Because it's constant for all classes, it's usually ignored in the classification step.*\n",
    "\n",
    "To make a prediction, the Naive Bayes classifier calculates the posterior probability for each class as the likelihood times the prior probability (often called just \"the prior\"). The class with the highest posterior probability is chosen as the predicted class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34aeff5",
   "metadata": {},
   "source": [
    "A Gaussian Naive Bayes classifier is a variant of the Naive Bayes classifier, specifically designed for continuous-valued features. It is based on Bayes' theorem and assumes that the likelihood of the features, given the class label, follows a Gaussian (normal) distribution. This means that each feature is assumed to be normally distributed within each class. The classifier can thus use the probability density function of the Gaussian distribution to compute the likelihood of a feature value given a class label.\n",
    "\n",
    "Basically, the classifier looks at the 2 (or more) blobs of data, and generates the Gaussian distribution that best fits the data, and then uses these along with the priors to calculate which distribution was most likely to have generated each data point. *The class with the highest posterior probability is chosen as the predicted class*.\n",
    "\n",
    "Despite the assumption of independence between features (in other words, that the data blobs represent uncorrelated data), the Gaussian Naive Bayes classifier often performs well in practice and can provide a good baseline for more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da793e-f4a7-4656-ba40-518d73898e94",
   "metadata": {},
   "source": [
    "## Classify our PCA-transformed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853f9947-b3f2-4db2-84e4-8da5d0f9be8b",
   "metadata": {},
   "source": [
    "First, we'll split our new PCA transformed data in to training and testing sets (as always)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa3b360-6b18-4ace-b6ae-819bf191d233",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a3f6a5-3eba-431d-a609-fec5929d0cbd",
   "metadata": {},
   "source": [
    "In the cell below, re-import the `train_test_split` method from `sklearn.model_selection` just to be sure the code below is stand-alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "35eaff39-6420-4f17-a95e-afe6bd001787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310bfa24-4a97-4708-8b18-7062c7f636c5",
   "metadata": {},
   "source": [
    "Now split the data into training and test sets. *Make sure to use the `X_pca` data, not the original data!* Use a `test_size = 0.3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "69f4c776-5086-4322-bbb4-bffa1289c7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the PCA-transformed data into \n",
    "# training and testing sets (70% training, 30% testing)\n",
    "# set random_state to 42 so we all get the same thing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a630be96-944e-4219-932e-7896466ccb13",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937a733d-c0ea-4b94-a526-fe8e7b4772be",
   "metadata": {},
   "source": [
    "Now we'll grab the Gaussian naive Bayes classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cf11efec-2b78-4283-a2e9-b1aaf8eb92b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea5760b-480c-4de7-818e-9ec043096a06",
   "metadata": {},
   "source": [
    "Once we create our classifier, it will behave just like all the others we have used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "08aba47e-d5a3-4ede-b00a-818dbbd6fcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Naive Bayes classifier\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfe7580-3c75-4e9a-9511-50f71ce0a0e2",
   "metadata": {},
   "source": [
    "We have now given birth to a classifier. We can train it using the `fit()` method, and then classify our test data using the `predict()` method, just as we have done before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f118d01a-b6ca-4930-ad08-46e9d4636415",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2d798c-e0cf-4024-a969-835c948f50a1",
   "metadata": {},
   "source": [
    "In the cell below, train our classifier using its `fit()` method using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9977d3bb-5a25-4ee8-87cf-117c2521ecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and train it on the PCA-transformed training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ce517-f025-4e40-a448-babf21755c5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70beb73-17d2-462c-9eef-d01681e46fa6",
   "metadata": {},
   "source": [
    "Now generate the predicted categories (targets) using the `predict()` method. Make sure and use the held-back `X_test` data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "901045c2-06f6-4a2d-b18a-9c01a29f3747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the PCA-transformed testing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb4ce04-4754-4b39-927b-394e65f2663c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95efcf08-f02b-495b-8323-44edbdde892a",
   "metadata": {},
   "source": [
    "Now we'll make a scatter plot of the data with mistakes as open symbols. Don't worry too much about the specific code, but you might want to keep handy for future plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20299aee-60cb-494e-b2c9-76fc01a19a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of the classified test data with mistakes as open symbols\n",
    "colors = ['red', 'green', 'blue']\n",
    "species = iris.target_names\n",
    "markers = ['o', 's', 'D']  # Different marker shapes for each class\n",
    "labels = ['Correct', 'Misclassified']\n",
    "\n",
    "# Plot the correctly classified points as filled symbols\n",
    "for i, color, target_name, marker in zip(range(3), colors, species, markers):\n",
    "    plt.scatter(X_test[(y_test == y_pred) & (y_test == i), 0], \n",
    "                X_test[(y_test == y_pred) & (y_test == i), 1], \n",
    "                color=color, marker=marker, \n",
    "                label=target_name if i == 0 else None)\n",
    "\n",
    "# Plot the misclassified points as open symbols\n",
    "for i, color, marker in zip(range(3), colors, markers):\n",
    "    plt.scatter(X_test[(y_test != y_pred) & (y_test == i), 0],\n",
    "                X_test[(y_test != y_pred) & (y_test == i), 1], \n",
    "                color=color, marker=marker, facecolors='none', \n",
    "                linewidths=1.5, edgecolors=color, \n",
    "                label=labels[1] if i == 0 else None)\n",
    "\n",
    "plt.xlabel(\"First Principal Component\")\n",
    "plt.ylabel(\"Second Principal Component\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Classified Test Data: Correct vs. Misclassified\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e4c537-df88-463f-aa7d-80421009cf78",
   "metadata": {},
   "source": [
    "Looks like we did pretty well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58340d-e8f3-42ca-a393-c20b31c277b0",
   "metadata": {},
   "source": [
    "Finally, let's compute the standard diagnostics for our classification.\n",
    "\n",
    "First, we'll import the methods from `sklearn.metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cf4e1c24-7f5e-4bcf-9d0c-1fe5176b3402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bcb2f2-f57a-4867-aa68-6edc5c07b301",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912cef5f-1dcb-42bf-af23-822cf8ae3fcc",
   "metadata": {},
   "source": [
    "Now, in the cells below, compute the accuracy score, classification report, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0b0e4e-5868-4810-a660-87c81b901ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d3a43b-95bc-4d8a-a7b1-bdafe61036d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d520002-299e-48a1-9b20-ebe7d5208b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the classification report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fb4329-1db9-440e-a746-2a1fd610b9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6382a6df-a7d5-4565-976c-63b3fa0d83f8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3755dfda-45c4-46a6-852b-7e61ceaec43f",
   "metadata": {},
   "source": [
    "And we're done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
