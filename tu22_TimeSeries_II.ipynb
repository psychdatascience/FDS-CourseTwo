{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4c2312c",
   "metadata": {},
   "source": [
    "# Time and Date Series in Python and Pandas II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49793308",
   "metadata": {},
   "source": [
    "In the previous tutorials we have learned how to use basic python libraries, seaborn, pandas, matplotlib etc. This tutorial uses time series to put together many of the previosu teachings in a practical example of data analysis.\n",
    "\n",
    "The tutorial is heavily inspired by a tutorial by Jake VanderPlas available in his [Python Data Science Handbook](https://www.oreilly.com/library/view/python-data-science/9781491912126/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50e5d52",
   "metadata": {},
   "source": [
    "#### Learning goals\n",
    "\n",
    "  - Manipulate real time series data\n",
    "  - Plot time series data\n",
    "  \n",
    "#### Prerequisites\n",
    "  - Python and NumPy\n",
    "  - Pandas, DataFrames and TimeSeries\n",
    "  - Seaborn\n",
    "  - Matplotlib\n",
    "  \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e85c1f",
   "metadata": {},
   "source": [
    "The goal of this tutorial is to put together many of the previous teahcings to analyze real world data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71af8c4",
   "metadata": {},
   "source": [
    "We will use open [data about hourly bicycle counts made available by the city of Seattle, WA](http://data.seattle.gov/). A copy of the dataset for this tutorial was sent before class.\n",
    "\n",
    "Seattle has a bridge called Fremont Bridge. The bridge has installed devices that count bycles passing over the bridge (an automated bicycle counter). The sensors are located in the east and west sidewalks of the bridge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62c191f",
   "metadata": {},
   "source": [
    "### Data loading and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8772b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set() # this will set seaborn as the default formatting for all plots\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6f7895",
   "metadata": {},
   "source": [
    "The first we will do is to load the data into a Pandas DataFrame. \n",
    "\n",
    "Make sure the data is saved inside a folder called 'datasets' saved in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516dadbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./datasets/Fremont_Bridge_Bicycle_Counter.csv', index_col='Date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e24af5",
   "metadata": {},
   "source": [
    "After loading the data, let's take a quick look at the DataFrame. Just the first few rows. In this way, we will also test if everything is loaded and ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78706c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4214b722",
   "metadata": {},
   "source": [
    "The labels in the columns of are a bit too long for our purposes. So we will simolify them into 'Total', 'West' and 'East'. These are the counts (how many bicycles) in the West or East sidewalks and the Total counts across the two sidewlaks, 'Total'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87698cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['Total','East', 'West']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fddf9d3",
   "metadata": {},
   "source": [
    "We will now recompute the total counts and save them back into the column called Total. \n",
    "\n",
    "Note. We use the method `eval`, that (ahem) evaluates a Python expression as a string. Meaning you can write a string, for a command and `eval` will run the command for you, like adding two columns of a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee942b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data\n",
    "data1['Total'] = data1.eval('West + East')\n",
    "data1.head()\n",
    "data1.dropna().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b20dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Total'] = data['West'] + data['East']\n",
    "data.head()\n",
    "data_tem = data.dropna()\n",
    "data_tem.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faa0105",
   "metadata": {},
   "source": [
    "We can now take a look at the reorganized columns. This time using `describe`. But we want to drop the missing entries, so we use first `dropna`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f20196",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db07997",
   "metadata": {},
   "source": [
    "Note the code above is a single line shortcut to a couple of operations. So we could have also written it in two steps. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd78c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = data.dropna()\n",
    "temp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bf7b54",
   "metadata": {},
   "source": [
    "Even though we have stressed the importance of being explicit with the coding, this single line instance is a helpful one. It shortens the code and it avoids creating the tempoary variable `temp`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6994d435",
   "metadata": {},
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb8bc93",
   "metadata": {},
   "source": [
    "The next thing we can do with the data is to visualize the time series. We can plot for example the counts (`Y` axis) by date (`X` axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef9fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot()\n",
    "plt.ylabel(\"Bicycle per hour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad860a50",
   "metadata": {},
   "source": [
    "Alright a lot of data here. From 2013 to 2022, up to thousands of bicycles counted.\n",
    "\n",
    "One way to approach TimeSeries of this density is to resample the. Pandas TimeSeries offer ways to resample the data say from days to weeks or from weeks to months, etc. The method is `DataFrame.resample()`, let's use it on our DataFrame, see hereafter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8870ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly = data.resample('W').sum()\n",
    "weekly.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c2e29c",
   "metadata": {},
   "source": [
    "We have now resampled the data by summing the counts at a weekly rate. So all the counts of  by day were summed for all days in a week. \n",
    "\n",
    "Let's now take a look at the plot of this data. Do you expect the values in the `Y` axis to be larger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09191ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly.plot(style=['-', '-', '-'])\n",
    "plt.ylabel('Weekly bicycle count');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2a1f60",
   "metadata": {},
   "source": [
    "The new plot shows a much larger number in the totals. There are differences between the west and east sidewalks. (More bicycles in the East?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75830135",
   "metadata": {},
   "source": [
    "Neat! That was easy. Pandas TimeSeries rock. Imagine having to find the weeks, and the counts associated to them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15701be",
   "metadata": {},
   "source": [
    "The plot above also seems to have some patterns in it. For example, we observe the data to go up and down for each of the traces. Possibly, in some part of each year more bicycles are counted. \n",
    "\n",
    "Can you guess when it might be that more bicycles are counted on this bridge in Seattle? (Guess this is not Texas.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9617d3ca",
   "metadata": {},
   "source": [
    "### Smoothing and simplifying the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1042a09b",
   "metadata": {},
   "source": [
    "It looked like the data had some pattern into it, perhaps higher bicycle counts in the better weather months. Yet, the data still showed quite a bit of fluctuations, noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea35904",
   "metadata": {},
   "source": [
    "Next, we will try to first make the data look smoother. One way to do that is to compute the mean of the counts over a certain period. That should in principle 'average out' some of the fluctuations across days for example. \n",
    "\n",
    "OK. To test this approach, \"average out\" the noise by averaging the counts we will do the following:\n",
    "\n",
    "  - Resample the data back to days.\n",
    "  - Compute an average, a rollling average, across the days in a month. \n",
    "  \n",
    "`DataFrame.rolling()` does precisely what we need. It averages across a certain number of data (say 15, or 30 days etc). \n",
    "\n",
    "We will first break this process down and then rewrite it into a single line command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b5f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = data.resample('D').sum()\n",
    "daily.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bc22ee",
   "metadata": {},
   "source": [
    "We resampled back to daily sums of the counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a877646f",
   "metadata": {},
   "source": [
    "Next, we are going to use `.rolling()` to compute an average over a period of 30 days, centered at each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d81b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = daily.rolling(30, center=True).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16439f5e",
   "metadata": {},
   "source": [
    "We saved the result in a new DataFrame called `temp`, we can take a peak at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21c150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd41a0c8",
   "metadata": {},
   "source": [
    "We have counts that are lower as we have averaged over 30 days. Neat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157caf82",
   "metadata": {},
   "source": [
    "Now we can plot the data and take a look at the new plot. Hopefully, we have simplified the visualization by 'averaging away' some of the smaller fluctuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e434c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.plot(style=['-', '-', '-'])\n",
    "plt.ylabel('mean hourly count');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac62074",
   "metadata": {},
   "source": [
    "Ok, looks a little bit better. Especially so for the West ad East data. We can try to get a smoother version by averaging using a gaussian window. This is a normal distribution that will wait more the counts close to the current day being averages and less and less, with a Gaussian distribution weighting the days further away from the current day (the day in the center of the window)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d5c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily.rolling(30, center=True,\n",
    "              win_type='gaussian').sum(std=10).plot(style=['-', '-', '-']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d32a16",
   "metadata": {},
   "source": [
    "Ok, even better. Note also how this time we avoinded using the intermediate variable `temp`, instead we averaged and plotted all in a single python line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50817f3a",
   "metadata": {},
   "source": [
    "To improve the smoothness we could additionally average over a longer period, more days, for example 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cada1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily.rolling(60, center=True,\n",
    "              win_type='gaussian').sum(std=10).plot(style=['-', '-', '-']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ac9a4a",
   "metadata": {},
   "source": [
    "OK, now, that looks really good. Yet, it still has some wiggle and even averaging over more days, it is not going to help it. Try, change the number of days to 120 or 180, is it better? Smoother?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a72e7a",
   "metadata": {},
   "source": [
    "### Looking at the data at higher resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955436bd",
   "metadata": {},
   "source": [
    "Smoothing the data was helpful but did not seem to provide deeper insights into the properties of this dataset. We were able to better see the same features that we saw earlier on in the dataset. Yet, we only saw the same features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c09e514",
   "metadata": {},
   "source": [
    "Let's try next to see if we can identify more features. To do so, let's look at the data in the opposite way, instead of smoothing over days, let's look at the data in a hournly fashion.\n",
    "\n",
    "We can use `DataFrame.groupby()` to get the data by time of the day (`btod`) and compute the mean over that time of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df7c55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_time = data.groupby(data.index.time).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e550ee29",
   "metadata": {},
   "source": [
    "Wel that was fast this was an average across all days, all weeks, all months, all years! Lot's of data, Pandas is pretty fast.\n",
    "\n",
    "Let's look at the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04e6fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_time.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6836024c",
   "metadata": {},
   "source": [
    "A much smaller dataset. Averaged across 24 hours... Neat. Simple. Fast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbcb29d",
   "metadata": {},
   "source": [
    "Let's plot the new dataset. To do so, given that we have data at an hourly rate. We will need to create some proper x-axis ticks, with the right numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884cf251",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_ticks = 4 * 60 * 60 * np.arange(6)\n",
    "by_time.plot(xticks=hourly_ticks, style=['-', '-', '-']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b2a921",
   "metadata": {},
   "source": [
    "Nice plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cdbe08",
   "metadata": {},
   "source": [
    "We learn here that there are a lot fo bicycles in the morning and in the afternoon. That makes sense. It must be that people in Seattle commute by bicycle. Excellent finding!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e88fb50",
   "metadata": {},
   "source": [
    "If we can see daily changes in counts due to communting, can we also see weekly changes? For example, would the weeked see less bicycles as less people go to work?\n",
    "\n",
    "We can use `groupby` to address and average the data weekly. Whereas earlier we had used `groupby` and indexed and averaged the data by time (`data.index.time`), here we can use the Padas TimeSeries index by days of the week, which is available (try `data.index.<TAB>` to see all available indices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a9e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_weekday = data.groupby(data.index.dayofweek).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ad4c09",
   "metadata": {},
   "source": [
    "OK similar operations, grouped and averaged the data by day of the week instead of by hours. This means that we averaged all Mondays, all Tueadays, All Wednesdays, etc.\n",
    "\n",
    "We now need to make a legend (by day of the week) and plot the new summary data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f462f4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_weekday.index = ['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun']\n",
    "by_weekday.plot(style=['-', '-', '-']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c2de33",
   "metadata": {},
   "source": [
    "As predicted, there is a strong difference between week days and weekend days. More, bicycles during the week days.\n",
    "\n",
    "Let's conclude. Make the two final plots organized together so to show the two patterns, daily bimodal distribution and weekly unimodal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f788e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "by_weekday.plot(ax=ax[0], title='Weekly',\n",
    "                          style=['-', '-', '-'])\n",
    "by_time.plot(ax=ax[1], title='Hourly',\n",
    "                       xticks=hourly_ticks, \n",
    "                       style=['-', '-', '-']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c81d32f",
   "metadata": {},
   "source": [
    "Let's be happy with this. We have used `matplotlib`, `seaborn`, and `pandas` to study a real dataset and evidence patterns of daily and weekly commute behavior of the peopel living in Seattle.\n",
    "\n",
    "Well done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa695a70",
   "metadata": {},
   "source": [
    "### Exercise.\n",
    "\n",
    "Look at any pattern across months? For example does the weather in the colder months affect the commute by bicycle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b79e7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
