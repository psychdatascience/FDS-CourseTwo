{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0417cec4-0aad-4258-9b73-945df37451d5",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c5b42-14d6-4314-af1e-6b428e19fcba",
   "metadata": {},
   "source": [
    "As we get into machine learning, we are going to run into some (perhaps) unfamiliar terms such as \n",
    "- \"optimization\" (which is pretty much what it sounds like),\n",
    "-  \"error\" (sort of what it sounds like),\n",
    "-   and \"cost\" (what it sounds like if you stretch your imagination).\n",
    "\n",
    "Fear not! Here, we shall unpack these terms so you have both a conceptual and working knowledge of what these concepts are. Then you can either \n",
    "- use algorithms you trust that use these terms,\n",
    "- or you can dig into the details of algorithms that use these terms and see what *you* think."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aa2705-077a-4188-93a5-129cd453c3b1",
   "metadata": {},
   "source": [
    "The term \"optimization\", as we just said, is almost what is sounds like: we're trying to make something (a machine learning algorithm) the very best that it can be! We want our \"model\" or our \"algorithm\" to be the greatest!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f3dce4-b3df-4f61-b166-e56c7c7dcaf9",
   "metadata": {},
   "source": [
    "But wait! How do we actually define \"optimum\"? Well, because we (literally, you and me) don't know *everything* there is to know about our data, then whatever we decide is \"optimum\" is going to be based on *intuition*, which we then turn into math (or Python functions... same thing -- sorry mathematicians).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbecce5-2f4e-4c80-b166-7a5901797bcc",
   "metadata": {},
   "source": [
    "## A simple and familiar example of optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9eacbd-2dd0-4e9b-8145-646fad88e699",
   "metadata": {},
   "source": [
    "Let's get the intuition for optimization by looking at a machine learning algorithm that you probably didn't even know was a machine learning algorithm: good ol' linear regression!  \n",
    "You might be wondering \"How is linear regression a machine learning?\" Good question! You might think of machine learning algorithms as, for example, classifying pictures of your face from all the other pictues you have. The key is that the machine learning algorithm is *predicting* which faces should be your face, and *predicting* that the other photos aren't. In other words, it is looking at all the values of the pixels in the images, and predicting whether those pixel values - the **data** - are correspond to your face or some other face. Well, this is pretty much what a regression does. A regression takes input data (the x values) and then, for any x we can think of, it makes a *prediction* of what the corresponding y value should be! For example, if we know that average happiness in a country is related to the health of the economy in that country as measured by gross domestic prodect (GDP), then we can predict the average happiness in *any* country, even those countries whose data weren't in the initial regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af0c997-dca5-4c42-8498-5454555418c8",
   "metadata": {},
   "source": [
    "## Optimization in regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047a8044-ee61-464e-ad95-192cea2dafbb",
   "metadata": {},
   "source": [
    "Let's look at a simple example using made-up (but realistic) data showing how happiness increases with increasing GDP. First, we'll import a couple libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e756fc-175c-4f9f-8e8c-0b49e0fce294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4604926f-65d6-4435-b49f-d04eee1eb718",
   "metadata": {},
   "source": [
    "Now let's make up some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0f3f3f-6db3-499e-bc42-75a66fa509ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data\n",
    "x = np.array([2, 4, 6, 8, 10])                  # GDP values\n",
    "data = np.array([1.8, 3.3, 3.7, 4.6, 6.7])      # happiness values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5997ac19-a435-4858-95a7-e5bae438505c",
   "metadata": {},
   "source": [
    "... and take a look at them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2874e474-bfe3-404e-9e5d-a2e65174d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data values\n",
    "plt.plot(x, data, 'ro')\n",
    "plt.plot(x, data, 'ro')\n",
    "plt.xlabel('Nation\\'s GDP')\n",
    "plt.ylabel('Nation\\'s Happiness')\n",
    "plt.title('Nation\\'s Happiness vs GDP')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d11daf5-1d10-45e0-8c2f-35910fa7b638",
   "metadata": {},
   "source": [
    "We can see that happiness does seem to increase with GDP. But can we come up with a good mathematical description of how happiness changes with GDP? If so, then we can use this description – this equation – to predict the overall happiness in any country we want? This is where intuition, thought, and maybe some trial and error come in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5550d099-9c01-4150-8720-90d31c72da7d",
   "metadata": {},
   "source": [
    "Looking at the data, I'd guess that a straight line with some positive slope might do a decent job of describing these data, so let's try it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0410ffe0-1a60-4822-a066-a7ce1fc5ad95",
   "metadata": {},
   "source": [
    "### Note: This is only a guess!  \n",
    "I don't have any theoretical reason for believing there is a straight-line relationship here; I'm just making a guess based on how the data look to me! I could have chosen any function, but a straight line is one of the simplest functions there is  \n",
    "*y* =  slope**x* + intercept  \n",
    "and we all learned it primary school anyway, so let's go with [Occam's razor](https://en.wikipedia.org/wiki/Occam%27s_razor) and use the simple straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12562b22-0317-432b-b85d-82d6d8d840d8",
   "metadata": {},
   "source": [
    "So let's make a straight line and plot it along with the data. First, we'll make an empty array to hold the predictions of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147cbab2-8f89-4031-bc13-a9ba50795ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldata = np.zeros(data.shape)                # array to hold model data values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74b434d-6e2e-4047-8169-532e2bf557f8",
   "metadata": {},
   "source": [
    "Now let's make some guesses for what the slope and y-intercept might be. Note I don't have any secret knowledge about what these values might be (do you?); *I'm just making a guess by looking at the data* – y seems to increase by about 1 unit when x changes from 2 to 4, for example. So that would be a rise of 1 for a run of 2, corresponding to a slope of rise/run = 1/2 or 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fed1f0-efee-43f1-8dbc-52ded55ab131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters/coefficients\n",
    "intercept = 0    # our guess for y-intercept\n",
    "slope = 0.5      # our guess for slope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea352ae-2feb-4b5e-a00d-a5660ff0f161",
   "metadata": {},
   "source": [
    "And now we can make our predicted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a0dcd0-e22e-4282-9871-0be17a9f5b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute model values\n",
    "for index, x_in in enumerate(x) :\n",
    "    y_out = intercept + slope*x_in # compute y value for this x\n",
    "    modeldata[index] = y_out       # and store in our array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dd7708-668f-4cfd-b8ef-511f83caae56",
   "metadata": {},
   "source": [
    "And plot them with the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c5827b-2cb4-41ae-884d-e857e2ea2c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model values on the same plot as the data\n",
    "plt.plot(x, data, 'ro')         # data\n",
    "plt.plot(x, modeldata, 'g-')    # model\n",
    "plt.xlabel('Nation\\'s GDP')\n",
    "plt.ylabel('Nation\\'s Happiness')\n",
    "plt.title('Nation\\'s Happiness vs GDP')\n",
    "plt.legend(['data', 'model'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a3870f-f39a-4b0f-b650-dc4c82141fb3",
   "metadata": {},
   "source": [
    "The model's attempt to fit the data is shown by the green line. It predicts a happiness of 1 when GDP is 2, 2 when GDP is 4, and so on... What do you think? How should we change the line in order to \"optimize\" it as a description of the data? Take a moment to ponder that question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b49c81-811f-45f8-953f-5895046e6c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as t\n",
    "a_moment = 5\n",
    "for i in range(a_moment) :\n",
    "    print('pondering...')\n",
    "    t.sleep(1)\n",
    "print('Done pondering!')\n",
    "print('The slope looks decent, but the y-intercept needs to be bigger to move the line up towards the data!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7607c3b5-fd20-4928-b97b-d969a41945b6",
   "metadata": {},
   "source": [
    "Hopefully you agree!  \n",
    "My thought process, and probably yours, went something like this:\n",
    "- The line is lower than *all* of the data points by similar amounts\n",
    "- If I imagine the same line higher up, it would pass through the data rather then being below them\n",
    "- As a result, all of the predicted values of the new model would be closer to the actual data than the current one\n",
    "- Our errors should get smaller overall "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b484e6b4-f3dd-4bb0-8eb7-160efe01dcb5",
   "metadata": {},
   "source": [
    "Based on that reasoning, let's change the y-intercept to something bigger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3552081b-04c4-4d50-a78b-0a6815a16b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b00e4-bc5d-46ed-b353-68506e32f955",
   "metadata": {},
   "source": [
    "Recompute the model values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea6a98c-0c92-42e6-bee2-8218ca88db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute model values\n",
    "for index, x_in in enumerate(x) :\n",
    "    y_out = intercept + slope*x_in # compute y value for this x\n",
    "    modeldata[index] = y_out       # and store in our array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce75eec2-343e-4780-b00a-96e09b64fbee",
   "metadata": {},
   "source": [
    "And plot the new model with the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735cc7a3-be64-467b-b6d0-90880dea8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model values on the same plot as the data\n",
    "plt.plot(x, data, 'ro')         # data\n",
    "plt.plot(x, modeldata, 'b-')    # model\n",
    "plt.xlabel('Nation\\'s GDP')\n",
    "plt.ylabel('Nation\\'s Happiness')\n",
    "plt.title('Nation\\'s Happiness vs GDP')\n",
    "plt.legend(['data', 'model with better intercept'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe94ca4-75fa-46f9-aead-f8634c6b4c65",
   "metadata": {},
   "source": [
    "Not bad! Take another moment to marvel at our work! I think we can all agree that the new line with the larger y-intercept does a better job of \"fitting\" the data than the first one. It seems to pass through the data more closely, and it doesn't miss the data points systematically like the first one did, undershooting everything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51754418-549c-44ed-9eaa-6acf3f3e30af",
   "metadata": {},
   "source": [
    "But – and here's a big \"but\" – how good is good enough? Could we change the y-intercept a little and do a little better? Would a slightly higher y-intercept be even more \"optimal\"? In order to settle this fight, we need to agree on what we mean by an algorithm being \"optimal\". Once we agree on this definition, we will all arrive at the exact same answer when we try to decide what is optimal!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ee5deb-b981-42d4-894f-8593507241dc",
   "metadata": {},
   "source": [
    "## Defining \"optimal\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb56514-095b-4177-8ba3-65fb03b692ce",
   "metadata": {},
   "source": [
    "When you see the work \"optimal\" you might think \"There can be only one optimum: whatever it is that is the very very best.\" You have to let go of that right now and leave it to the philosophers and mathameticians. For our purposes, optimum means the parameter values of our model – in this case the slope and y-intercept – that come closest to all the data values such that *any change would make the model worse*. When you get to the point where any change in your guess for the parameters makes your model worse, you have found the \"optimal\" set of parameter values that define our line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1ae430-bbfb-4217-830e-bfe3efd90d26",
   "metadata": {},
   "source": [
    "Thus, \"optimal\" in this case is just the slope and y-intercept that makes the line pass the closest to all the data points – any change from these \"optimal\" values will make our fit of the data worse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75707d9-c5bc-4192-a516-260c85273768",
   "metadata": {},
   "source": [
    "At this point, we've defined optimum in terms of \"better\" and \"worse\", but what does it mean to be a \"better\" fit or a \"worse\" fit. We need to define exactly what we mean by this. To do so, we'll introduce the concept of **error** in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1bae0c-f84a-4909-8892-b987a6b60828",
   "metadata": {},
   "source": [
    "## Defining \"error\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d769fa8-8e50-4ac8-bfa7-8a6a3c3594d8",
   "metadata": {},
   "source": [
    "Our exact definition of error generally depends on the specific situation, but it is always some calulation, hopefully a simple one, of how well the model is capturing the data. In this situation, to calculate how much our model misses the data, we can just calculate *how far each predicted value misses each data point*. That's it!  \n",
    "So let's compute the errors for an intercept of 0 and add them to our plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09162f07-ac2c-4dfb-894b-094f3a35be1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = 0    # set our intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a679eb1-d092-4006-afcc-da339ebc2bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, x_in in enumerate(x) :    # recompute our model values\n",
    "    y_out = intercept + slope*x_in\n",
    "    modeldata[index] = y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f204796-af6d-4f2a-becd-7969e9fef240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data values\n",
    "plt.plot(x, data, 'ro')\n",
    "plt.xlabel('Nation\\'s GDP')\n",
    "plt.ylabel('Nation\\'s Happiness')\n",
    "plt.title('Nation\\'s Happiness vs GDP')\n",
    "\n",
    "# plot the model values and errors on the same plot as the data\n",
    "plt.plot(x, data, 'ro')         # data\n",
    "plt.plot(x, modeldata, 'g-')    # model\n",
    "\n",
    "# Add vertical lines to represent errors - this is the only new code!\n",
    "for i in range(len(x)):\n",
    "    plt.vlines(x[i], modeldata[i], data[i], colors='g', linestyles='dashed')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18b24f2-f42e-4160-b366-e2301205716b",
   "metadata": {},
   "source": [
    "The errors are shown by the green dashed lines. The size of the error is length of each of these lines, which is just *each data value minus each model value*. So we can just compute them like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121d695c-1037-4b8a-b939-1f196bfaff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = data - modeldata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ff69eb-5570-4f7c-bf69-32281e3a33a7",
   "metadata": {},
   "source": [
    "Now we can look at the errors themselves by plotting then across our GDP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131a4c39-86e5-436a-8e53-3f48710ed102",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, errors, 'go-')\n",
    "plt.ylim([0, 2])\n",
    "plt.ylabel('prediction error')\n",
    "plt.xlabel('GDP')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7eae41-cbd8-44a9-949d-70d59ea9ccf2",
   "metadata": {},
   "source": [
    "Here we can notice that 1) all the errors are positive, meaning that our model is undershooting all the data, and 2) the errors range in size from about 0.6 or so up to about 1.75. \n",
    "Now let's make the same plot of errors for our the better y-intercept and compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f588f0d-4f15-4f59-bc9d-cbc557964d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = 1    # our new, improved y intercept\n",
    "\n",
    "# Compute model values\n",
    "for index, x_in in enumerate(x) :\n",
    "    y_out = intercept + slope*x_in\n",
    "    modeldata[index] = y_out\n",
    "\n",
    "better_errors = data - modeldata   # compute the new errors\n",
    "\n",
    "# plot everybody\n",
    "plt.plot(x, errors, 'go-')\n",
    "plt.plot(x, better_errors, 'bo-')\n",
    "plt.ylabel('prediction error')\n",
    "plt.xlabel('GDP')\n",
    "plt.legend(['original line', 'line with better intercept'])\n",
    "plt.ylim([-2, 2])\n",
    "plt.axhline(y=0, color='k', linestyle='--')    # reference line at error = 0\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcab90a-c6cc-49b2-bbe4-4ee3eb237cf3",
   "metadata": {},
   "source": [
    "Note that for the y-intercept of 1 (blue), the errors contain both undershoots and overshoots, and they cluster around zero (the black dashed line) whereas the line with y-intercept of zero (green), overshoots all the values and the *overall values of the errors* are substantially larger (the *largest* error for our blue curve is about the *smallest* error of the green curve!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962f5d5e-7dd4-4e16-9a9a-cd41731865bd",
   "metadata": {},
   "source": [
    "How would we summarize the errors for each curve in order to compare them? One thing we could do is just compute the sum of the errors (or their average). The problem with that is that we want to count negative errors the same as positve errors – we don't want them to cancel each other out!  \n",
    "The common solution to this, which you probably already know, is to square the errors before summing them. This is known as the *summed square error* or **SSE**. You could also average them, which is known as the *mean squared error* or **MSE**.  \n",
    "In machine learning, such an overall error metric is called the **cost**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e648664d-0146-4df8-a432-5dd4646c6d1e",
   "metadata": {},
   "source": [
    "## Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73568a8a-522e-4fd6-968c-437706611d48",
   "metadata": {},
   "source": [
    "**cost**: a single number summary of the errors between model and data. Everybody loves money, so our goal is to find the cheapest possible line – the one that has the lowest *cost*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884554c9-7130-4725-ba39-a3bdfa49997b",
   "metadata": {},
   "source": [
    "Since cost is a number directly calculated from the errors, we can now say that *the \"optimal\" solution* – the best line that describes our data – *is the one that yields the lowest **cost***. The process of finding this lowest cost is called *minimization* or *optimization*. There are tons of algorithms tailored to do minimization, many of them quite fancy using theorems from calculus and whatnot but – don't worry! – it's not hard to get the basic idea!  \n",
    "To get this idea, let's plot the cost of various straight lines with different y-intercepts (let's assume that we have the slope correct for simplicity so we don't have to worry about it).  \n",
    "The code to compute and plot the cost for various y-intercepts is below.\n",
    "- look through it, it's all based on bits of code from above so it should be easyish to understand\n",
    "- then run the to see the plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c7be17-29ad-4ed5-8900-7748c0bc7e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compute the cost for various y-intercepts and plot the cost vs. y-intercept result\n",
    "'''\n",
    "\n",
    "# Make data\n",
    "x = np.array([2, 4, 6, 8, 10])                  # x values\n",
    "data = np.array([1.8, 3.3, 3.7, 4.6, 6.7])      # data values\n",
    "\n",
    "# number of intercepts to try\n",
    "num_intercepts = 100\n",
    "int_range = [0, 2]\n",
    "\n",
    "# Model parameters/coefficients\n",
    "slope = 0.5                                                           # slope is constant - we're assuming we have it right\n",
    "intercepts = np.linspace(int_range[0], int_range[1], num_intercepts)  # make num_intercepts y-intercepts\n",
    "\n",
    "costs = []    # empty array to hold the costs\n",
    "\n",
    "# Loop over intercepts and compute cost\n",
    "for intercept in intercepts:             # loop through the candidate intercepts\n",
    "    modeldata = intercept + slope * x    # compute the model predictions\n",
    "    errors = data - modeldata            # ... and the errors\n",
    "    cost = np.sum(errors**2)             # compute cost as SSE\n",
    "    costs.append(cost)                   # store the cost for this y-intercept\n",
    "\n",
    "# Plot cost vs. y-intercept\n",
    "plt.plot(intercepts, costs, 'b-o', alpha = 0.3)\n",
    "plt.xlabel('Y-Intercept')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost vs. Y-Intercept')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45374141-71e2-4c7c-aba5-a5185423dc01",
   "metadata": {},
   "source": [
    "Aha! This looks pretty! When our y-intercept is too high, the cost gets super high, When the y-intercept is too low, the cost also gets super high. And we can also see that there is an optimal solution – a minumum or low-spot in the curve that is [\"just right\"](https://en.wikipedia.org/wiki/Goldilocks_and_the_Three_Bears), in that whichever way we try to change the y-intercept, the cost goes up! This is our optimum – our \"best-fitting\" value to capture these data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c1c74e-d945-4806-bfa0-00ac38019bc7",
   "metadata": {},
   "source": [
    "To get the actual minimum cost, we can use NumPy's `min()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515a66f2-70af-48d0-b0e8-a29bab47f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'the minimum cost is {np.min(costs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29388e75-124d-4807-bec0-69057b50f00d",
   "metadata": {},
   "source": [
    "And to get the y-intercept at which the minumum occurs, we can use NumPy's handy `argmin()` function, which returns the index at which the minumum occurs. We can then use this index to grab the corresponding intercept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8505f2-677d-400c-8e3f-8d443997e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cost_idx = np.argmin(costs)    # this could all be a one-liner inside print...\n",
    "min_yint = intercepts[min_cost_idx]\n",
    "print(f'the y-intercept for the minimum cost is {min_yint}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d52335-a515-43ef-b602-b8570aeb9227",
   "metadata": {},
   "source": [
    "Thus, it seems like our guess of \"1\" for the intercept that we made above wasn't too bad! Based on the candidate intercepts, the winner was 1.0101, which is pretty darn close to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec60b536-95b3-4e93-adb5-667200bdd2c2",
   "metadata": {},
   "source": [
    "## (But what about the slope?) Optimization in general"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b38be3-9742-4613-a9ab-c29e04e0578d",
   "metadata": {},
   "source": [
    "Remember that to calculate the optimal y-intercept, we assumed we had the slope about right so we wouldn't have to worry about it. But that's cheating, right? In most situations, we'd need to find the best combination of y-intercept *and* slope that best describe the data at hand (and that we would then use to predict new values, which is what machine learning is for!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3544299e-8ca8-4335-b9ce-174224fa0369",
   "metadata": {},
   "source": [
    "The code below will make a plot of our costs vs. many combinations of both slope and y-intercept. Do do this, we have to compute 100^2 combinations, rather than just the 100 we did above when we were only changing the y-intercept. If we had 3 parameters we were trying to fit, it would be 100^3 combinations! So having more parameters in our model means many many many more computations! This is why mathameticians and computer scientists work so hard on optimation algorithms – they are generally very hard and – wait for it – costly. Here's the code (don't worry if the plotting looks mysterious):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e66176c-834c-43ff-a46a-efd8a95b9c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Make data\n",
    "x = np.array([2, 4, 6, 8, 10])                  # our GDP values\n",
    "data = np.array([1.8, 3.3, 3.7, 4.6, 6.7])      # happiness values\n",
    "\n",
    "# Model parameters/coefficients\n",
    "intercepts = np.linspace(0, 2, 100)  # y-intercepts ranging from 0 to 2\n",
    "slopes = np.linspace(0.25, 0.75, 100)  # slopes ranging from 1/4 to 3/4\n",
    "\n",
    "# Create a meshgrid for intercepts and slopes\n",
    "intercepts_grid, slopes_grid = np.meshgrid(intercepts, slopes)\n",
    "\n",
    "# Initialize an array to hold the costs\n",
    "costs = np.zeros(intercepts_grid.shape)\n",
    "\n",
    "# Loop over intercepts and slopes and compute cost\n",
    "for i in range(intercepts_grid.shape[0]):\n",
    "    for j in range(intercepts_grid.shape[1]):\n",
    "        intercept = intercepts_grid[i, j]\n",
    "        slope = slopes_grid[i, j]\n",
    "        modeldata = intercept + slope * x\n",
    "        errors = data - modeldata\n",
    "        cost = np.sum(errors**2)\n",
    "        costs[i, j] = cost\n",
    "\n",
    "# Plot cost vs. y-intercept and slope\n",
    "fig1 = plt.figure()\n",
    "ax1 = fig1.add_subplot(111, projection='3d')\n",
    "ax1.plot_surface(intercepts_grid, slopes_grid, costs, cmap='viridis')\n",
    "ax1.view_init(elev=10, azim=100) # set the view angle\n",
    "ax1.set_xlabel('Y-Intercept')\n",
    "ax1.set_ylabel('Slope')\n",
    "ax1.set_zlabel('Cost')\n",
    "ax1.set_title('Cost vs. Y-Intercept and Slope')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdddb89-f667-4dad-a4e4-f0914080500e",
   "metadata": {},
   "source": [
    "Well, this is interesting! It looks like there are combinations of intercepts and slopes that yield big costs, but also many combinations that look like they cost the same (the large diagonal trough in the surface). What gives? I would say I was surprised, but after playing with optimization for 30 years, I'm no longer surprised by anything!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46264d8-1ba7-4ca6-8084-6979878d8941",
   "metadata": {},
   "source": [
    "One thing that could be happening is that there *is* a difference in the cost values along the trough, but that they are too small to see. To examine this, let's plot the *logarithm* (`np.log(costs)`) instead of costs. This will magnify the differences among small costs, while compressing the difference between large costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f582be2f-0fdf-45ed-b4bb-b8ec4cfd49c2",
   "metadata": {},
   "source": [
    "Here's the code to make the plot. The only difference is that the y-axis is log(cost) instead of just cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a0e9dc-5ce4-4033-9307-4a96cecbc7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot log cost vs. y-intercept and slope\n",
    "fig2 = plt.figure()\n",
    "ax2 = fig2.add_subplot(111, projection='3d')\n",
    "log_costs = np.log(costs)\n",
    "ax2.plot_surface(intercepts_grid, slopes_grid, log_costs, cmap='viridis')\n",
    "ax2.view_init(elev=10, azim=100) # set the view angle\n",
    "ax2.set_xlabel('Y-Intercept')\n",
    "ax2.set_ylabel('Slope')\n",
    "ax2.set_zlabel('Log(Cost)')\n",
    "ax2.set_title('Log(Cost) vs. Y-Intercept and Slope')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c8cad8-c8b8-4997-971c-2576fdd246be",
   "metadata": {},
   "source": [
    "Ah! Now we can see that there is indeed a bottom to the trough: a true minumum cost. The minimum cost looks to occur at a slope of about 0.5 and and a y-intercept between 0.5 and 1. Notice that our best-fit y-intercept looks like it changed a bit when we let the slope vary to find the optimal solution as well! This tells us that we can't look for the best value of our parameters one-at-a-time – the best value of a parameter will often depend on the value of other parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7be47a-a205-4345-b29b-dd1277c1ad96",
   "metadata": {},
   "source": [
    "The code below finds and prints our optimal solution: the best values for slope, y-intercept, and (minimum) cost that they produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb06fa89-0370-4855-8c51-32ec779fad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index of the minimum cost\n",
    "min_cost_index = np.argmin(costs)\n",
    "min_cost_intercept = intercepts_grid.flatten()[min_cost_index]\n",
    "min_cost_slope = slopes_grid.flatten()[min_cost_index]\n",
    "min_cost = costs.flatten()[min_cost_index]\n",
    "\n",
    "print(f\"The minimum cost is {min_cost}\")\n",
    "print(f\"The y-intercept for the minimum cost is {min_cost_intercept}\")\n",
    "print(f\"The slope for the minimum cost is {min_cost_slope}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64249af5-88d2-4edc-b747-361aea028c05",
   "metadata": {},
   "source": [
    "So we ended up with a slope of 0.55 (our original guess we made by-eye wasn't too bad!), and a y-intercept of 0.71. To predict new data values from new countries, we can now just plug that countries GDP into the equation  \n",
    "*y* = 0.55 * *GDP* + 0.71  \n",
    "and we have out machine learning prediction for happiness in this country!  \n",
    "Obviously, if our original data were representative and our assumption about a straight-line relationship was valid, or predictions should be pretty accurate! If not, we go back and figure out what went wrong. We're the original \"training\" data bad? Did we pick the wrong model? What?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2f4872-c8c3-4627-b29f-a4aeefbb0759",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c736f8e9-d27d-4b91-b41f-e7c47ac51e80",
   "metadata": {},
   "source": [
    "In order to do \"machine learning\", we first have to \n",
    "- pick a model (we used a straint line relationship)\n",
    "- \"train\" the model on some data (use optimization to find the parameters of your model that best fit the training data)\n",
    "- use these \"optimal\" parameters to predict future data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b62ef49-9086-4a9a-9e98-ffa886748b8f",
   "metadata": {},
   "source": [
    "Here, we concentrated on the middle step, which is conceptually often the most difficult. We concentrated on a \"1D\" optimization problem (varying only a single parameter – y-intercept). The 1D optimization can teach us what we really need to know about optimization. It is simply a search for a parameter value that yields the lowest overall error (the cost) between our model predictions and our data. But we also now appriciate how dealing with multiple parameters can yield much higher complexity and computing time, which is why we all owe mathameticians and computer scientists a debt of gratitude for working on fancy optimization algorithms!)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
