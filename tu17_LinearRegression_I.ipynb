{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71412ba3-e995-41b1-8eed-f4605c1353e6",
   "metadata": {},
   "source": [
    "# Linear Regression I\n",
    "\n",
    "Learning outcomes:\n",
    "\n",
    "  - Basic linear regression concepts\n",
    "  - Implement model fit using matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbc3658",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Linear regression is a fundamental concept in statistics and machine learning. \n",
    "\n",
    "Linear regression is used to model the relationship between a dependent variable and one or more independent variables. It assumes that there is a linear relationship between the independent variable(s) and the dependent variable. The goal of linear regression is to find the best-fit line that minimizes the difference between the predicted values and the actual values.\n",
    "\n",
    "Linear regression is important because it can help us to understand and predict the relationship between two or more variables. It is widely used in many fields such as economics, finance, engineering, and social sciences. For example, in finance, linear regression can be used to predict stock prices based on economic indicators. In engineering, it can be used to predict the strength of materials based on their composition. In social sciences, it can be used to understand the relationship between poverty and crime rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5ab7dc",
   "metadata": {},
   "source": [
    "When we fit a regression model using linear regression, we are trying to find a line (it applies also to curves) that best represents the relationship between the independent variable(s) (say `x`) and the dependent variable (say `y`). The amount of variation in the dependent variable that can be explained by the independent variable(s) is determined by the fit of the regression line to the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7cce61",
   "metadata": {},
   "source": [
    "A generalized equation to model via linear regression is the following:\n",
    "\n",
    "$Y=\\alpha+\\beta*X$\n",
    "\n",
    "Where $X and Y$ are arrays containing the data points, `\\alpha` and `\\beta` parameters of the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a76551",
   "metadata": {},
   "source": [
    "Linear regression can also be used as a starting point for more complex models. For example, if we find that a linear relationship exists between two variables, we can use this information to build more complex models that incorporate additional variables or non-linear relationships. Linear regression is also a useful tool for hypothesis testing and determining the significance of relationships between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b273c8f7",
   "metadata": {},
   "source": [
    "The simplest form of this general linear regression model is a line. Below we wills tart with a line to demonstrate how we can fit via linear regression using only `NumPy` and then we will dig a little bit deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a821d",
   "metadata": {},
   "source": [
    "--- \n",
    "### A simple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cb1bc9",
   "metadata": {},
   "source": [
    "In Python, we can perform linear regression using the `numpy` and `matplotlib` libraries (or using `scikit-learn`but that will be for a later stage). \n",
    "\n",
    "Below an example of how to do it. First, we need to import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f80b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd9690e",
   "metadata": {},
   "source": [
    "Next, we need to create some sample data to work with. Let's say we want to establish a relationship between the number of hours studied and the grades obtained by students. We can create a NumPy array with some random data like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778001b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20])\n",
    "y = np.array([60, 80, 90, 92, 94, 94, 96, 98, 98, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b4b617",
   "metadata": {},
   "source": [
    "Here, we've created two `NumPy` arrays: `x`, which contains the number of hours studied, and `y`, which contains the corresponding grades obtained by the students.\n",
    "\n",
    "Now, we can calculate the slope and intercept of the linear regression line using `NumPy`'s `polyfit()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58f8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#slope, intercept = np.polyfit(x, y, 1)\n",
    "coeff = np.polyfit(x, y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c154ee4d",
   "metadata": {},
   "source": [
    "The above calculated the `slope` and `intercept` of the linear regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first coefficient is the slope of the line\n",
    "# The second coefficient is the intercept\n",
    "slope = coeff[0]\n",
    "intercept = coeff[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fb923f",
   "metadata": {},
   "source": [
    "We can now plot our data and the linear regression line to visualize the relationship between the two variables. First though, we will need to cunstruct out line, the line described by `slope` and `intercept`. This line is defined for each data point. \n",
    "\n",
    "As it turns out `NumPy` has a nice way to do so. The function `polyval` can be used by passing the output (`coeff` in our case, but any output) of `polyfit` to obtain the predicted `y` values of a fit given the corrsponding `x` datapoints (not the `y`, we are predicting those): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c8e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear regression model\n",
    "y_hat = np.polyval(coeff, x)\n",
    "\n",
    "# y_hat = slope*x + intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35775c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.plot(x, y_hat, color='red')\n",
    "plt.ylabel('Grade (scale 0-100)')\n",
    "plt.xlabel('Hours of study time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904db9fd",
   "metadata": {},
   "source": [
    "The above is a scatter plot of our data points and plot the linear regression line through them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77dbd05",
   "metadata": {},
   "source": [
    "The red line, is our **model** the best fitting line to the data. To determine the best fitting line `polyfit` has adjusted the intercept and slope of a line so as to minimize the distance between the model and the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a166f",
   "metadata": {},
   "source": [
    "We can see what it is going on here. For each data point the line returned by `polyfit` has a predicted `y_hat` point (that is the normal way we call predicted points `y_hat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6417ba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and the fit\n",
    "plt.scatter(x, y, label='data')\n",
    "plt.scatter(x, y_hat, label='y_hat', color='red')\n",
    "plt.plot(x,y_hat, color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53b0d7c",
   "metadata": {},
   "source": [
    "To fit the line `polyfit` had to find the `intercept` and `slope` that minimized the distance between the data point (blue dots) and the corresponding location on the line (red dots). The final values of `slope` and `intercept` were `11.631` and `72.3`, correspondingly in our case.\n",
    "\n",
    "To find these points `polyfit` coputed the *sum-of-squares* between the data (`y`) and the corresponding points on the line. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7ba44e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Great! So, so far, we have learned that we can use `polyfit` to fit a line (likely we can done more, hint *poly-* stands for multiple) and `polyval` to extract the prediction of the line on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcea434",
   "metadata": {},
   "source": [
    "---\n",
    "### Fitting good models (a.k.a. Model selection)\n",
    "\n",
    "Given a dataset it is not always clear what model to fit to it. In general, we do not know before fitting a model whether it is a good or a bad model. R² and similar measures can be used to identify good, bad, or at least better or worse models.\n",
    "\n",
    "A good model would have a high R², a bad model a low R², etc. So oftentimes R² is used as a criteria to judge wheter a model in linear regression is better or worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f81195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset with quadratic trend\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-5, 5, num=20)\n",
    "y = 2*X**2 - X + 1 + np.random.normal(scale=5, size=len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfe7711",
   "metadata": {},
   "source": [
    "After creating our dataset we will fit a line to it, using `polyfit` and `polyval`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e380d0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear model\n",
    "lin_coeffs = np.polyfit(X, y, deg=1)\n",
    "linear_fit = np.polyval(lin_coeffs, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e48052",
   "metadata": {},
   "source": [
    "Let's plot the result, data and line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa9364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and fit\n",
    "plt.scatter(X, y, label='data')\n",
    "plt.plot(X, linear_fit, label='linear fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fa9a3b",
   "metadata": {},
   "source": [
    "Mmh, it does not looke like a good model. Why? Well the data seem to curve and the line, oh well, is linear.\n",
    "\n",
    "---\n",
    "\n",
    "We can use `polyfit` to fit a higher order model. For example, instead of a line, we can fit a quadratic term (a parabola is a quadratic term). This can be done by changing the degree parameter, when calling the function. \n",
    "\n",
    "Above `deg` was set to `1`. To make it quadratic we will need to set it to `2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fedf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a quadratic function\n",
    "quad_coeffs = np.polyfit(X, y, deg=2)\n",
    "quadratic_fit = np.polyval(quad_coeffs, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44912f65",
   "metadata": {},
   "source": [
    "We now plot this quadratic term and the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4531f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, label='data')\n",
    "plt.plot(X, quadratic_fit, label='quadratic fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7f99f0",
   "metadata": {},
   "source": [
    "Alright, things look much better in this case. The quadratic function represents the data much, much better than the line. So this is a good model in this case!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d44aba",
   "metadata": {},
   "source": [
    "---\n",
    "### Two ways to be a bad model (i.e., not fitting or overfitting the data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a0c838",
   "metadata": {},
   "source": [
    "Most models are wrong. \n",
    "\n",
    "We have encountered a bad model, above. The line through was not a good model; it did not represent the data well.\n",
    "\n",
    "Let's look at it again in a slightly different way to understand what was happening when fitting the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ce5d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and fit\n",
    "plt.scatter(X, y, label='data')\n",
    "plt.plot(X, linear_fit, label='linear fit', color='red')\n",
    "plt.scatter(X, linear_fit, label='linear fit', color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1f1cc",
   "metadata": {},
   "source": [
    "For each data point (blue) `polyfit` identify a corresponding model point (red).\n",
    "\n",
    "We decide that the line is not a good model because the symbols from the data (blue) and those generated by the model (red) are far from each other. \n",
    "\n",
    "The quadratic model instead, does a much better job, meaning that the model generates points that pass much closer to the data points.\n",
    "\n",
    "This is a classic case of a bad (linear) and good (quadratic) model, judging from the perspective of the *quality of fit*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23101991",
   "metadata": {},
   "source": [
    "--- \n",
    "When judging the quality of fit to the data of a model, things can go wrong in at least a couple of ways. One way was encountered above. The line was not a good model, in a certain way it was too rigid and did not accomodate the curve in the data.\n",
    "\n",
    "Another possible way the model fitting process can go wrong can appear a little bit counter intinitive at first, but it is very important. If a model is too flexible for the data, the model can be fit to the data but it will not be a good model.\n",
    "\n",
    "let's take a look."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb91d54",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's imagine a very flexible model, a model that can represent the data very well. Or better, a model that can represent *each data point* very well, but miss the overall pattern in the data. In other words a model that can miss the forest for the trees.\n",
    "\n",
    "We will test a very flexible model. Instead of a line `deg=1`, or a quadratic `deg=3`, we can test a model that has a much higher degrees of freedom say `deg=12` (not very important if 10, 11, 12, 13, etc., as long as it is a much larger number than 2 or 3 so as to make it into a very flexible model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db502128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a higher-order polynomial\n",
    "overfit_coeffs = np.polyfit(X, y, deg=12) # What happens if deg > 20 (the size of the data vector) and what type of error is returned by python and why\n",
    "overfit_fit = np.polyval(overfit_coeffs, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246122d1",
   "metadata": {},
   "source": [
    "Let's plot this model fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c2e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and the fits\n",
    "plt.scatter(X, y, label='data')\n",
    "plt.plot(X, quadratic_fit, label='quadratic fit')\n",
    "plt.plot(X, overfit_fit, label='overfit fit', color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15112c38",
   "metadata": {},
   "source": [
    "As we can appreciate from the plot the higher-order model (red) does follow the data well, but it actually follows each datapoint too much and while capturing the curve in the data it does not summarize it well. it has a bunch of distracting wiggly movements that we are not sure we should commit trusting.\n",
    "\n",
    "For example, what if we were to generate new data in the same way? It would have some new, random variations and each datapoint would move a little bit up and down, but we would expect the overal curve not to change shape.\n",
    "\n",
    "Let's try this experiment: Make new data using the same process used above but letting new variability to each data point. After that we will **plot** (not fit) the quadratic and higher-order models from above. \n",
    "\n",
    "Given that the data-generation process is not changing (only the tiny random fluctuations are, the noise) we can evaulate how the two models fit to the previous dataset fit the new dataset. \n",
    "\n",
    "A good model should fit the trends in the data well. Independently of random fluctuations in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240749d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new dataset with quadratic trend\n",
    "np.random.seed(100)\n",
    "X_2 = np.linspace(-5, 5, num=20)\n",
    "y_2 = 2*X_2**2 - X_2 + 1 + np.random.normal(scale=5, size=len(X_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4987ce95",
   "metadata": {},
   "source": [
    "Plot the new dataset with the quadratic model from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9605642",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_2, y_2, label='data')\n",
    "plt.plot(X, quadratic_fit, label='quadratic fit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd62214",
   "metadata": {},
   "source": [
    "OK Confirmed. The datapoints are different. Yet the parabola seems to be doing a pretty good job to representing the new dataset. \n",
    "\n",
    "---\n",
    "\n",
    "Next let plot on top of the new data the higher-order model fit above, to the previous dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b70fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_2, y_2, label='data')\n",
    "plt.plot(X, overfit_fit, label='overfit fit', color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a30876c",
   "metadata": {},
   "source": [
    "Ah! This time the model does not seem to be following each individual datapoint well. This is because this model was following each individual data point in the first dataset too much. It was as this phenomena generally described **overfitting** the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c02a5ed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In sum, a regression model can be a good model when it captures important trends in the data, without following too close any of the minor variations in the data. If we want to use an analogy, a good model does not msis the forest fro the trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cdcdab",
   "metadata": {},
   "source": [
    "---\n",
    "### Getting more technical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c525da",
   "metadata": {},
   "source": [
    "Define:\n",
    "-- Mean Squared Errors\n",
    "-- residuals\n",
    "-- R2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87d910f",
   "metadata": {},
   "source": [
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "MSE stands for Mean Squared Error, which is a widely used loss function for regression problems. It measures the average squared difference between the predicted and actual values. Here's the formula for MSE:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1810643b",
   "metadata": {},
   "source": [
    "$MSE = 1/n * ∑(y - ŷ)²$\n",
    "\n",
    "where:\n",
    "\n",
    "  - $n$ is the number of observations\n",
    "  - $y$ is the actual value\n",
    "  - $ŷ$ is the predicted value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a71927",
   "metadata": {},
   "source": [
    "In this example, we first generate some random data using NumPy. Then, we fit a linear regression model using the np.polyfit function, which returns the coefficients of the linear regression line. We use these coefficients to make predictions on the input data using the np.polyval function.\n",
    "\n",
    "Finally, we compute the MSE between the actual values of y and the predicted values using NumPy's np.mean function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e8ad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate random data\n",
    "np.random.seed(123)\n",
    "x = np.random.rand(50)\n",
    "y = 2*x + 0.5*np.random.randn(50)\n",
    "\n",
    "# Fit a linear regression model\n",
    "coeffs = np.polyfit(x, y, 1)\n",
    "y_pred = np.polyval(coeffs, x)\n",
    "\n",
    "# Compute MSE\n",
    "mse = np.mean((y - y_pred)**2)\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e5573",
   "metadata": {},
   "source": [
    "The MSE reports the average squared difference between the predicted and actual values. \n",
    "\n",
    "The MSE is use to fit models and evaluate models.\n",
    "\n",
    "Below, we plot the data and the regression line using Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8337039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and the regression line\n",
    "plt.scatter(x, y, label='Data')\n",
    "plt.plot(x, y_pred, color='r', label='Linear regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbb03ec",
   "metadata": {},
   "source": [
    "#### Residuals\n",
    "\n",
    "Residuals are the differences between the actual values of the dependent variable and the predicted values of the dependent variable. \n",
    "\n",
    "In the context of MSE, the residuals are the differences between the observed values y and the predicted values ŷ. Residuals are useful for evaluating the performance of the model and checking for patterns or trends that the model may have missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aba38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate random data\n",
    "np.random.seed(123)\n",
    "x = np.random.rand(50)\n",
    "y = 2*x + 0.5*np.random.randn(50)\n",
    "\n",
    "# Fit a linear regression model\n",
    "coeffs = np.polyfit(x, y, 1)\n",
    "y_pred = np.polyval(coeffs, x)\n",
    "\n",
    "# Compute residuals\n",
    "residuals = y - y_pred\n",
    "print(\"Residuals:\", residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19580df",
   "metadata": {},
   "source": [
    "In this example, we first generate some random data using NumPy. Then, we fit a linear regression model using the np.polyfit function and make predictions on the input data using the np.polyval function. Finally, we compute the residuals as the difference between the actual values of y and the predicted values ŷ. The residuals are printed to the console.\n",
    "\n",
    "After model fitting, it is important to examine the residuals to ensure that they are randomly distributed around zero and do not exhibit any patterns or trends. One way to visualize the residuals is to create a scatter plot of the residuals against the predicted values. If the residuals are randomly distributed around zero, the plot should not exhibit any patterns or trends. If there is a pattern or trend, it suggests that the model is not capturing some aspect of the data and may need to be adjusted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33b17fe",
   "metadata": {},
   "source": [
    "This code creates a scatter plot of the residuals against the predicted values, with a red line at y=0 to indicate where the residuals should be centered around. A well-fitted model should have a scatter plot of residuals that is relatively evenly distributed around the red line. If there are any patterns or trends, this may indicate that the model is not capturing some aspect of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c42969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create scatter plot of residuals\n",
    "plt.scatter(y_pred, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c95f76",
   "metadata": {},
   "source": [
    "### Linear regression using scikit-learn (generalized linear regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6dbd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb14fcb",
   "metadata": {},
   "source": [
    "Next, we need to create some sample data to work with. Let's say we want to establish a relationship between the number of hours studied and the grades obtained by students. We can create a NumPy array with some random data like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3267516",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20]).reshape((-1, 1))\n",
    "y = np.array([60, 80, 90, 92, 94, 94, 96, 98, 98, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18733e23",
   "metadata": {},
   "source": [
    "Here, we've created two NumPy arrays: x, which contains the number of hours studied, and y, which contains the corresponding grades obtained by the students.\n",
    "\n",
    "Now, we can create a LinearRegression object and fit our data to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dc887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ef5ab3",
   "metadata": {},
   "source": [
    "This created a linear regression model and fit it to our data.\n",
    "\n",
    "We can now use the predict() method of our model to make predictions for new data points. For example, if we want to predict the grades of students who have studied for 15 hours, we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1870a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = np.array([15]).reshape((-1, 1))\n",
    "y_new = model.predict(x_new)\n",
    "print(y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e3c713",
   "metadata": {},
   "source": [
    "The above outputted the predicted grade for a student who has studied for 15 hours.\n",
    "\n",
    "We can also plot our data and the linear regression line to visualize the relationship between the two variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d47a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb42244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
