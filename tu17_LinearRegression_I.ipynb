{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71412ba3-e995-41b1-8eed-f4605c1353e6",
   "metadata": {},
   "source": [
    "# Linear Regression I\n",
    "\n",
    "Learning outcomes:\n",
    "\n",
    "  - Basic linear regression concepts\n",
    "  - Implement model fit using matplotlib\n",
    "  - Compute Sum of Squared Errors\n",
    "  - Model fit quality \n",
    "  - Overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbc3658",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Linear regression is a fundamental concept in statistics and machine learning. \n",
    "\n",
    "Linear regression is used to model the relationship between a dependent variable and one or more independent variables. It assumes that there is a linear relationship between the independent variable(s) and the dependent variable. The goal of linear regression is to find the best-fit line that minimizes the difference between the predicted values and the actual values.\n",
    "\n",
    "Linear regression is important because it can help us to understand and predict the relationship between two or more variables. It is widely used in many fields such as economics, finance, engineering, and social sciences. For example, in finance, linear regression can be used to predict stock prices based on economic indicators. In engineering, it can be used to predict the strength of materials based on their composition. In social sciences, it can be used to understand the relationship between poverty and crime rates.\n",
    "\n",
    "Importantly, the term **linear** in Linear regression does not refer only to a line, but it applies to any polynomial! Linear here is referred to the parameters of the model and not the model *per se*!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5ab7dc",
   "metadata": {},
   "source": [
    "Indeed, when we fit a regression model using linear regression, we are trying to find the set of parameters of a function that best represents the relationship between the independent variable(s) (say `x`) and the dependent variable (say `y`). \n",
    "\n",
    "The amount of variation in the dependent variable that can be explained by the independent variable(s) is determined by the fit of the regression line to the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7cce61",
   "metadata": {},
   "source": [
    "A generalized equation for linear regression is the following:\n",
    "\n",
    "$Y=\\alpha+\\beta*X+\\epsilon$\n",
    "\n",
    "Where $X$ and $Y$ are arrays containing the data points, $\\alpha$ and $\\beta$ parameters of the regression model. \n",
    "\n",
    "The last term $\\epsilon$ is called error but it is not a mistake, it is the randomness in the universe that we are never going to capture. The whole goal of linear regression, and any modelling in general, is to capture the data and disregard the noise. See the forest and without focusing too much on the trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b273c8f7",
   "metadata": {},
   "source": [
    "The simplest form of this general linear regression model is a line. Below we will start with a line to demonstrate how we can fit via linear regression using only `NumPy` and then we will dig a little bit deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a821d",
   "metadata": {},
   "source": [
    "--- \n",
    "### A simple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cb1bc9",
   "metadata": {},
   "source": [
    "In Python, we can perform linear regression simply using the `numpy` and `matplotlib` libraries (or using more advanced libraries such as  `scikit-learn` or `statsmodels`, we will focus on the simpler cases here). \n",
    "\n",
    "Below is an example of how to use `numpy`'s functions `polyfit` and `polyval` to fit and evaluate a linear regression model. \n",
    "\n",
    "First, we will import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f80b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd9690e",
   "metadata": {},
   "source": [
    "Next, we need to create some sample data to work with. Let's say we want to establish a relationship between the number of hours studied and the grades obtained by students. We can create a `NumPy` array with some random data like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "778001b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20])\n",
    "y = np.array([60, 80, 90, 92, 94, 94, 96, 98, 98, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b4b617",
   "metadata": {},
   "source": [
    "Here, we've created two `NumPy` arrays: \n",
    "\n",
    "  - `x`, which contains the number of hours studied, and \n",
    "  - `y`, which contains the corresponding grades obtained by the students.\n",
    "\n",
    "Now, we will calculate the slope and intercept of a straight line fit to the data using `NumPy`'s `polyfit()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e58f8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#slope, intercept = np.polyfit(x, y, 1)\n",
    "coeff = np.polyfit(x, y, deg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c154ee4d",
   "metadata": {},
   "source": [
    "The above calculated the `slope` and `intercept` of the linear regression line. The first entry in `coeff` is the `slope`, the second the `intercept`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first coefficient is the slope of the line\n",
    "# The second coefficient is the intercept\n",
    "slope = coeff[0]\n",
    "intercept = coeff[1]\n",
    "print(f\"{slope=:.2f}, {intercept=:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fb923f",
   "metadata": {},
   "source": [
    "We can now plot our data and the linear regression line to visualize the relationship between the two variables. \n",
    "\n",
    "The function `polyval` can be used to plot evaluate the results of `polyfit`. The function takes as input the entire array returned by `polyfit` and the `x` values and returns the predicted `y` values, we will call these last, `y_hat`. \n",
    "\n",
    "So for each `y` data point there will be a corresponding `y_hat` predicted by the straight line: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4c8e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear regression model\n",
    "y_hat = np.polyval(coeff, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11c0130",
   "metadata": {},
   "source": [
    "We can now plot the results and the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35775c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.plot(x, y_hat, color='red')\n",
    "plt.ylabel('Grade (scale 0-100)')\n",
    "plt.xlabel('Hours of study time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904db9fd",
   "metadata": {},
   "source": [
    "The above is a scatter plot of our data points and plot the linear regression line through them. This is the equation of the line we just fitted:\n",
    "\n",
    " $\\hat{y} = slope*x + intercept$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77dbd05",
   "metadata": {},
   "source": [
    "The red line, is our **model** the best fitting line to the data. \n",
    "\n",
    "To determine the best fitting line `polyfit` has adjusted the intercept and slope of a line so as to minimize the distance between the model and the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a166f",
   "metadata": {},
   "source": [
    "We can see what it is going on here. For each data point the line returned by `polyfit` has a predicted `y_hat` point. (In general, *predicted* values are denoted mathematically with a \"^\" above them, a \"hat\", so in code we write things like \"y_hat\" instead, which is the way it's pronounced anyway.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6417ba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and the fit\n",
    "plt.scatter(x, y, label='data')\n",
    "plt.scatter(x, y_hat, label='y_hat', color='red')\n",
    "plt.plot(x,y_hat, color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268abd97",
   "metadata": {},
   "source": [
    "To fit the line `polyfit` had to find the `intercept` and `slope` that minimized the distance between the data point (blue dots) and the corresponding location on the line (red dots). \n",
    "\n",
    "The final values of `slope` and `intercept` were `1.631` and `72.3`, correspondingly in our case.\n",
    "\n",
    "To find these points `polyfit` computed the *sum-of-the-squared-error* (SSE) between the data (`y`) and the corresponding points on the line. \n",
    "\n",
    "The distance between the Y_hat and the Y we call it error ($\\epsilon$). We call it error because we hope it is random variation that we do not need to capture. So if the fit is good the overall error should be as small as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b48899b",
   "metadata": {},
   "source": [
    "Next, let's compute the error at each datapoint. This is the difference between `y` (the data to be predicted) and `y_hat` (the prediction from the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d52de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the numbers\n",
    "print(y-y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19fbd08",
   "metadata": {},
   "source": [
    "Now, as we discussed above to fit the model `polyfit` had to minimize these numbers. One way to do so is to sum all of them together and minimize the total sum of errors. \n",
    "\n",
    "So let's take a look at the total sum of errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c75771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sum of errors is small\n",
    "print(sum(y-y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadeb124",
   "metadata": {},
   "source": [
    "Mhm, that is a very small number. Good I guess, but it seems too small given that the blue dots and red dots are far away from each other, and the errors above show that. What is going on here?\n",
    "\n",
    "The reason the sum is small is because the negative errors and the positive errors for each individual datapoint tend to cancel each others out. \n",
    "\n",
    "A better measure of fit of a model to data to use during linear regression, the one commonly used when fitting or evaluating models is the the sum of squared errors or SSE. This is because if we square the errors, now the positives and negatives differences will *not* cancel out, there are no negatives anymore because of the squared operation. So, the total sum of the errors will be bigger.\n",
    "\n",
    "Let's first compute the individual squared errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0da99b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((y-y_hat)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f731d",
   "metadata": {},
   "source": [
    "Ok, bigger numbers, all positive this time. That is important. \n",
    "\n",
    "Next, let's take the sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96d9e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sum of square is big \n",
    "\n",
    "print(sum((y-y_hat)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53b0d7c",
   "metadata": {},
   "source": [
    "Now, that is a bigger number. The sum of all positive numbers returned a big number, better...\n",
    "\n",
    "So, as a summary so far, we have learned about:\n",
    "- linear regression and more specifically \n",
    "- how to fit a line using `NumPy` `polyfit` and `polyval` and \n",
    "- a metric for evaluating models, such as the sum of squared errors (SSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d9e57",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color:blue\">Note on *polyfit* and *polyval*:</span> \n",
    "\n",
    "The functions `polyfit` and `polyval` in numpy use implement fitting and evaluation of polynomials.\n",
    "\n",
    "Polynomials are math expressions with one or more algebraic terms. The word \"polynomial\" contains the Greek root *poly-* which means *many* and *-nomial* which means *term*.\n",
    "\n",
    "A polynomial are defined by constants (numbers), variables (letters), and positive exponents (powers). \n",
    "\n",
    "*(There are never negative exponents in polynomials.)*\n",
    "\n",
    "The standard form for a polinomial is the following:\n",
    "    \n",
    " * First order polinomial or a straight line. This polynomial has no curve: $y= a*x + b$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeac9edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.linspace(-5, 5, num=20)\n",
    "fit = np.polyval([1,1], X)\n",
    "plt.plot(X, fit, label='1st Order Polynomial')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92f3d2d",
   "metadata": {},
   "source": [
    " * Second order polinomial has a quadratic term and is also called parabola. This polynomial has one curve: $y= a*x^2 + bx + c$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40f7b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = np.polyval([1,1,1], X)\n",
    "plt.plot(X, fit, label='2nd Order Polynomial')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998f41c8",
   "metadata": {},
   "source": [
    " * Third order polinomial has a cubic term. This polynomial has two curves: $y= a*x^3 +bx^2 + cx + d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7fe53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = np.polyval([1,1,1,1], X)\n",
    "plt.plot(X, fit, label='3rd Order Polynomial')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78533f8",
   "metadata": {},
   "source": [
    "More terms can be added by adding additional exponents:  \n",
    " - $y= a*x^4 +bx^3 + cx^2 + dx + e$\n",
    " - $y= a*x^6 +bx^5 + cx^4 + dx^3 + ex^2 + fx +g$\n",
    " - Etc.\n",
    " \n",
    "The larger the number of exponents, the larger the number of curves the polynomial can have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ab635f",
   "metadata": {},
   "source": [
    "Next let's practice how to use `polyfit` and `polyval`.\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:blue\">Exercise</span> \n",
    "\n",
    "Use the following data and practice the following:\n",
    "- fit a straight line through the data.\n",
    "- make a plot of the data and the line.\n",
    "- report whether the fit looks good-enough and explain why you decided that way\n",
    "- report a quantitative measure to back up your claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a729bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your new data\n",
    "e_x = np.array([2.2, 1.9, 1.6, 0.8, 0.1, -0.1, -0.75, -1.6, -1.9, -2.2, -3, -3.2, -3.8, -4.2, -4.7])\n",
    "e_y = np.array([2, -1, 5, 0.1, 0, 5, 1, 10, 18, 15, 20, 36, 38, 45, 58])\n",
    "\n",
    "# Fit the data with a polynomial of degree one\n",
    "\n",
    "# Evaluate the polynomial to extract the straight line\n",
    "\n",
    "# plot data (red) and straight line (blue)\n",
    "\n",
    "# Comment on the quality of the fit\n",
    "\n",
    "# compute a measure of the quality of the fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcea434",
   "metadata": {},
   "source": [
    "---\n",
    "### Fitting good regression models (a.k.a. Model selection)\n",
    "\n",
    "In most cases, it is not trivial to identify a good model to fit to data. This is because most often, we do not have good reasons to know the model to fit to the data before actually doing so. So, what datascientists do is the following: \n",
    "   (1) Look at the data, i.e., make a plot of the data.\n",
    "   (2) Try to fit a simple model (the simplest we can think of generally).\n",
    "   (3) Try to fit a (one or more) more complex model. \n",
    "   (4) Evaluate the quality of the fit to compare the models\n",
    "   \n",
    "A good model should have a *low SSE*, a bad model a *high SSE*, etc. So often times metrics like the SSE (there are others we will learn about later) are used as a criteria to judge whether a regression model is better or worse than others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332f06f5",
   "metadata": {},
   "source": [
    "This is not the moment to endure a deep study of the best practices for model selection and the metrics used for that. Instead, what we will do is to fit a couple of different models to a new dataset.\n",
    "\n",
    "First, we will generate some new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f81195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset with quadratic trend\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-5, 5, num=20)\n",
    "y = 2*X**2 - X + 1 + np.random.normal(scale=5, size=len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfe7711",
   "metadata": {},
   "source": [
    "We will then fit a straight line to the dataset, using `polyfit` and `polyval`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e380d0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear model\n",
    "lin_coeffs = np.polyfit(X, y, deg=1)\n",
    "line_fit = np.polyval(lin_coeffs, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e48052",
   "metadata": {},
   "source": [
    "We can now plot the result, data and line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa9364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and fit\n",
    "plt.scatter(X, y, label='data')\n",
    "plt.plot(X, line_fit, label='linear fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a41eff",
   "metadata": {},
   "source": [
    "Mmh, it does not look like a good model. Why? Well the data seem to curve and the line, oh well, is straight. We will need to try a different model a more curvy one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7d4ec1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can use `polyfit` to fit a higher order model, this would still be a linear regression but fitting linear parameters for a curve. \n",
    "\n",
    "For example, instead of a line, we can fit a quadratic term (a parabola is a quadratic term). This can be done by changing the degree parameter when calling the function. \n",
    "\n",
    "Above `deg` was set to `1`. To make it quadratic we will need to set it to `2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fedf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a quadratic function\n",
    "quad_coeffs = np.polyfit(X, y, deg=2)\n",
    "quadratic_fit = np.polyval(quad_coeffs, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44912f65",
   "metadata": {},
   "source": [
    "We now plot this quadratic term and the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4531f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, label='data')\n",
    "plt.plot(X, quadratic_fit, color='red', label='quadratic fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265b3d48",
   "metadata": {},
   "source": [
    "Alright, things look much better in this case. The quadratic function represents the data much, much better than the line. So this is a good model in this case!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7f99f0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "To make our visual assessment a little bit more quantitative, let's compute the SSE for the line model and the quadratic model.\n",
    "\n",
    "To do so, we will actually define a function that computes the SSE and call that function twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445e4154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sse(y_hat, y):\n",
    "    # Compute the Sum of Squared Errors (SSE)\n",
    "    sse = sum( (y-y_hat)**2 )\n",
    "    return sse\n",
    "\n",
    "line_sse = sse(line_fit, y)\n",
    "print(\"the line model SSE is\", line_sse, \".\")\n",
    "\n",
    "quadratic_sse = sse(quadratic_fit, y)\n",
    "print(\"the quadratic model SSE is \", quadratic_sse, \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a934e3fe",
   "metadata": {},
   "source": [
    "Ok the SSE is MUCH larger for the line than for the quadratic model. This is a nice way to see how well SSE can work in distinguishing good from bad models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dd25d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As a summary so far, we have learned how to fit straight-line as well as a quadratic, we have learned also how to compute the SSE and to use the SSE to evaluate the uality of fit of a model.\n",
    "\n",
    "Next we will explore another situation, where a very good model ends up being a pretty bad one; overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a53889",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### <span style=\"color:blue\">Exercise</span> \n",
    "\n",
    "Use the above data and practice the following:\n",
    "- fit a third order polynomial to `X`\n",
    "- plot the second order and the third order polynomials (as lines) with the data (as symbols) using a single plot but different colors\n",
    "- Compute the SSE of the second- and third-order polinomial fit\n",
    "- explain which fit is best and how you came to such conclusion\n",
    "- (harder and not necessary) what are the equations being used by `polyval`/`polyfit` for a the first, second and third order polynomials?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d380009c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36d44aba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Two ways to be a bad model (i.e., not fitting or overfitting the data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a0c838",
   "metadata": {},
   "source": [
    "Most models are wrong *(Anonymous, 2023 circa)*. \n",
    "\n",
    "We have encountered a bad model, above. The line through the data was not a good model, the quadratic was.\n",
    "\n",
    "Let's look at it again in a slightly different way to understand what was happening when fitting the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ce5d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and fit\n",
    "plt.scatter(X, y, label='data')\n",
    "plt.plot(X, line_fit, label='linear fit', color='red')\n",
    "plt.scatter(X, line_fit, label='linear fit', color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59e7444",
   "metadata": {},
   "source": [
    "For each data point (blue) `polyfit` identify a corresponding model point (red).\n",
    "\n",
    "We decide that the line is not a good model because the symbols from the data (blue) and those generated by the model (red) are far from each other. \n",
    "\n",
    "The quadratic model instead, does a much better job, meaning that the model generates points that pass much closer to the data points.\n",
    "\n",
    "This is a classic case of a bad (straight line) and good (quadratic) model, judging from the perspective of the *quality of fit*. Indeed, we found that the SSE was small for the quadratic and larger for the line. The SSE is a measure of quality of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23101991",
   "metadata": {},
   "source": [
    "When judging the quality of fit to the data of a model, things can go wrong in at least a couple of ways. One way was encountered above. The line was not a good model, in a certain way it was too rigid and did not accomodate the curve in the data.\n",
    "\n",
    "Another possible way the model fitting process can go wrong can appear a little bit counter intinitive at first, but it is very important. If a model is too flexible for the data, the model can be fit to the data but it will not be a good model; the model might get the trees but miss the forest.\n",
    "\n",
    "Let's take a look."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb91d54",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's imagine a very flexible model, a model that can represent the data very well. Or better, a model that can represent *each data point* very well, but miss the overall pattern in the data. In other words a model that can miss the forest for the trees.\n",
    "\n",
    "We will test a very flexible model. \n",
    "\n",
    "Instead of a line `deg=1`, or a quadratic `deg=2`, we can test a model that has a much higher degrees of freedom say `deg=12` (not very important if 10, 11, 12, 13, etc., as long as it is a larger than 3 and smaller than 19, we will learn later why 19):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db502128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a higher-order polynomial\n",
    "overfit_coeffs = np.polyfit(X, y, deg=12) # What happens if deg > 20 (the size of the data vector) and what type of error is returned by python and why\n",
    "overfit_fit = np.polyval(overfit_coeffs, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246122d1",
   "metadata": {},
   "source": [
    "Let's plot this model fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c2e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and the fits\n",
    "plt.scatter(X, y, label='data')\n",
    "plt.plot(X, quadratic_fit, label='quadratic fit')\n",
    "plt.plot(X, overfit_fit, label='overfit', color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b9d149",
   "metadata": {},
   "source": [
    "As we can appreciate from the plot the higher-order model (red) does follow the data well, but it actually follows each datapoint too much. It does capture the curve in the data but it does not summarize it as well as the quadratic fit. \n",
    "\n",
    "This is because, it has a bunch of distracting wiggly movements that we are not sure we should commit trusting. It misses a little bit of forest (the global curve in the data), because it follows the individual trees (the data points) too much. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c60bc1a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### <span style=\"color:blue\">Exercise</span> \n",
    "\n",
    "Use the above data and practice the following:\n",
    "- fit a 12th order polynomial to `X`\n",
    "- plot the second order and the 12th-order polynomials (as lines) with the data (as symbols) using a single plot but different colors\n",
    "- Compute the SSE of the second- and 12th-order polinomial fit\n",
    "- explain which fit is best and how you came to such conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8637170b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "175f295a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15112c38",
   "metadata": {},
   "source": [
    "If we think this situation in terms of SSE only, the 12th-order polynomial does have a lower SSE. So, what is it going on here and how can we think about this situation?\n",
    "\n",
    "Let's take a step back and consider that a linear regression fit is not meant to return a fit that *only* represent the current data at hand. Instead, a fit is meant to represent an underlying feature of the data and not the noise. Remember the equation encoutered early on:\n",
    "\n",
    "$Y=\\alpha+\\beta*X+\\epsilon$\n",
    "\n",
    "\n",
    "The fit is supposed to characterize $Y$ but not $Y+\\epsilon$, what is a very flexible model end up fitting $Y$ and $\\epsilon$? \n",
    "\n",
    "How can we test this? What if we were to generate new data in the same way this dataset was generated? It would have some new, random variations (noise: $\\epsilon$) and each datapoint would move a little bit up and down, but we would expect the overal curve not to change shape.\n",
    "\n",
    "Let's try this experiment: \n",
    "\n",
    "- Make new data using the same process used above but letting new variability to each data point. \n",
    "- After that we will **plot** (not fit) the quadratic and 12th-order models from above.\n",
    "- We will then compare their quality of fit, just like done above.\n",
    "\n",
    "Given that the data-generation process is not changing (only the tiny random fluctuations are, the noise) we can evaulate how the two models fit to the previous dataset fit the new dataset. \n",
    "\n",
    "A good model should fit the trends in the data well. Independently of random fluctuations in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240749d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new dataset with quadratic trend\n",
    "np.random.seed(100)\n",
    "X_2 = np.linspace(-5, 5, num=20)\n",
    "y_2 = 2*X_2**2 - X_2 + 1 + np.random.normal(scale=5, size=len(X_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4987ce95",
   "metadata": {},
   "source": [
    "Plot the new dataset with the quadratic model from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9605642",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_2, y_2, label='data', color='black')\n",
    "plt.plot(X, quadratic_fit, label='quadratic fit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9f9144",
   "metadata": {},
   "source": [
    "The parabola seems to be doing a pretty good job to representing this new dataset (which trust me, we generated using the same process). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd62214",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next let plot on top of the new data the higher-order model fit above, to the previous dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b70fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_2, y_2, label='data', color='black')\n",
    "plt.plot(X, overfit_fit, label='overfit fit', color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a30876c",
   "metadata": {},
   "source": [
    "Ah! This time the higher-order model does not seem to follow each individual data point very well. \n",
    "\n",
    "This is because this model was fit to, and flexibly adjusted to follow, the individual data points in the first dataset. \n",
    "\n",
    "You have just encountered one of the major phenomena in linear reression and model fitting: **overfitting.** If a model is too flexible it can fit the data and the noise and that reduces generalization, meaining that the model does an OK job at fitting the original dataset but it does not generalize to new datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c02a5ed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In sum, a regression model can be a good model when it captures important trends in the data, without following too close any of the minor variations in the data. \n",
    "\n",
    "If we want to use an analogy, a good model *does not miss the forest for the trees*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f6f504",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### <span style=\"color:blue\">Exercise</span> \n",
    "\n",
    "Use the following data and practice the following:\n",
    "- Plot data1 vs data2 to show the difference.\n",
    "- Compute SSE of the quadratic model using X_2.\n",
    "- Compute SSE of the higherorder model using X_2.\n",
    "- explain which fit is best and how you came to such conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3776ad53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
